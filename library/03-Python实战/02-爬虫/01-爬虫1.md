爬虫

> - 基本操作
> - 高性能相关（Socket+select）
>   - twisted
>   - tornado
>   - gevent
> - Web版本微信（练习）
> - Scrapy框架（学习规则）
> - 自己爬虫框架

1. 爬虫基本操作

   URL指定内容获取到

   - 发送HTTP请求：URL
   - 基于正则表达式获取内容

   requests和bs4中的beautifulsoap（识别html标签）

   ```python
   import requests
   from bs4 import beautifulsoup
   response = request.get('http://xxxxxx')
   # python内置的html解析器：html.parse
   obj = beatifulsoup(response.text, 'html.parser')
   标前对象 = obj.find('a') # 找到匹配的成功的第一个标签
   标前对象 = obj.find_all('a') # 找到匹配的成功的多个标签，list
   xxx.find(name='标签名',id='id名称')
   参数：name，id，class_
   因为class和python内部关键字冲突，所以加一个小的下划线，或者
   attrs = {
       # 这里唯独不能写标签名称
       'class': 'xxx',
       'id': 'xxx'
   }
   
   pip3 install BeautifulSoup4
   ```

   response.text这个是字符串，response.content这个是字节，可以字节转。

   或者设置response.encoding=gbk。

```python
import requests 
from bs4 import BeautifulSoup  
response = requests.get('http://www.autohome.com.cn/news/')  
response.encoding='gbk'
soup = BeautifulSoup(response.text, 'html.parser') 
li_soup = soup.find(id='auto-channel-lazyload-article').find_all(name='li')
In [11]: for li in li_soup:
    ...:     print(li.find('h3'))
<h3>或退出中国 铃木正与长安商议解除合资</h3>
<h3>均降0.80万元 瑞迈柴油版全系车型调价</h3>
<h3>起售价降至44.38万 国产普拉多全系官降</h3>
None
<h3>从商用车切入 大众/福特将组成战略联盟</h3>
<h3>奥迪S3小心了 AMG A 35 L运动轿车谍照</h3>
<h3>6月25日亮相/三季度上市 吉利缤瑞消息</h3>
<h3>百人口碑评新车：中华V6大气外观得好评</h3>
<h3>汽车之家电动汽车挑战赛开场哨即将吹响</h3>
…………………………
```

记住这里是可能存在None的，因为实际的页面中，可能穿插着各种广告什么的，这些都是正常的，所以对于这种为None的就可以忽略了，因此在循环中可以做一个判断`if not li`然后直接continue就行了。

我们去到的列表中的元素每一个都是一个bs4.element.Tag的对象，因此如果要直接取内容的话直接用li.find('h3').text就行了。

找标签属性：

```python
# 得到一个属性的字典
In [15]: for li in li_soup:
    ...:     title = li.find('a')
    ...:     if not title:
    ...:         continue
    ...:     print(title.attrs)
    ...:     
{'href': '//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624'}
{'href': '//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624'}
{'href': '//www.autohome.com.cn/news/201806/918828.html#pvareaid=102624'}
{'href': '//www.autohome.com.cn/news/201806/918827.html#pvareaid=102624'}

##### 一样的效果
In [17]: for li in li_soup:
    ...:     title = li.find('a')
    ...:     if not title:
    ...:         continue
    ...:     print(title.attrs['href'])
    ...:     
    ...:     
//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624
//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624

# 或者使用get
In [16]: for li in li_soup:
    ...:     title = li.find('a')
    ...:     if not title:
    ...:         continue
    ...:     print(title.get('href'))
    ...:     
    ...:     
//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624
//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624
//www.autohome.com.cn/news/201806/918828.html#pvareaid=102624
//www.autohome.com.cn/news/201806/918827.html#pvareaid=102624
//www.autohome.com.cn/news/201806/918821.html#pvareaid=102624
```

找图片：

```python
In [18]: for li in li_soup:
    ...:     title = li.find('img')
    ...:     if not title:
    ...:         continue
    ...:     print(title.attrs['src'])
    ...:     
    ...:     
    ...:     
//www2.autoimg.cn/newsdfs/g26/M03/39/41/120x90_0_autohomecar__ChcCP1spq8KARebOAAGJFNHdNlQ952.jpg
//www2.autoimg.cn/newsdfs/g26/M02/39/0D/120x90_0_autohomecar__wKgHEVspsPaASndQAAFOtCp6URI106.jpg
```

如果要把图片下载下来呢？

```python
import requests
from bs4 import BeautifulSoup
response = requests.get('http://www.autohome.com.cn/news/')
response.encoding='gbk'
soup = BeautifulSoup(response.text, 'html.parser')
li_soup = soup.find(id='auto-channel-lazyload-article').find_all(name='li')

for li in li_soup:

    img = li.find('img')
    if not img:
        continue
    url = img.attrs['src']
    file_name = url.rsplit('/')[8]
    url = 'http:%s' % url
    res = requests.get(url)
    with open('/Users/lamber/tmp/%s' % file_name, 'wb') as f:
        # content就是字节类型，text是文本类型
        f.write(res.content)
```

## 自动登录github的示例

1. 登录页面发送请求，获取csrftoken
2. 发送post请求，携带用户名密码，csrf token。登录成功以后返回的内容一定是有cookie的。

```python
# https://github.com/login
import requests
from bs4 import BeautifulSoup

# 获取token
r1 = requests.get('https://github.com/login')
s1 = BeautifulSoup(r1.text, 'html.parser')
token = s1.find('input', attrs={'name': 'authenticity_token'}).get('value')


# 以post形式发送用户名密码以及csrf_token
"""
其实每个网站要向后台提交的数据是不一样的，如果不知道就可以去模拟测试一下，打开firebug，点击发送以后
查看网络请求到底发了什么，github在提交的时候需要以下这些参数
- utf8
- authenticity_token
- login: 标签的name等于login
- password：这里也是标签的name属性。
- commit: Sign in
"""
r2 = requests.post(
    'https://github.com/session',
    data={
        'utf8': '✓',
        'authenticity_token': token,
        'login': '1020561033@qq.com',
        'password': '13082171785',
        'commit': 'Sign in'
})
# r2.cookies这是一个对象，如果要拿到字典类型的cookie，可以继续调用get_dict()方法
cookies_dict = r2.cookies.get_dict()

r3 = requests.get(
    url='https://github.com/settings/profile',
    cookies=cookies_dict
)
print(r3.text)
```

