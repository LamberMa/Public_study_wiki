if(typeof AWPageMounts=='undefined'){AWPageMounts={}};AWPageMounts['m17']=[{"name":"01-爬虫1.md","path":"17-爬虫/01-爬虫1.md","content":"爬虫\n\n> - 基本操作\n> - 高性能相关（Socket+select）\n>   - twisted\n>   - tornado\n>   - gevent\n> - Web版本微信（练习）\n> - Scrapy框架（学习规则）\n> - 自己爬虫框架\n\n1. 爬虫基本操作\n\n   URL指定内容获取到\n\n   - 发送HTTP请求：URL\n   - 基于正则表达式获取内容\n\n   requests和bs4中的beautifulsoap（识别html标签）\n\n   ```python\n   import requests\n   from bs4 import beautifulsoup\n   response = request.get(\'http://xxxxxx\')\n   # python内置的html解析器：html.parse\n   obj = beatifulsoup(response.text, \'html.parser\')\n   标前对象 = obj.find(\'a\') # 找到匹配的成功的第一个标签\n   标前对象 = obj.find_all(\'a\') # 找到匹配的成功的多个标签，list\n   xxx.find(name=\'标签名\',id=\'id名称\')\n   参数：name，id，class_\n   因为class和python内部关键字冲突，所以加一个小的下划线，或者\n   attrs = {\n       # 这里唯独不能写标签名称\n       \'class\': \'xxx\',\n       \'id\': \'xxx\'\n   }\n   \n   pip3 install BeautifulSoup4\n   ```\n\n   response.text这个是字符串，response.content这个是字节，可以字节转。\n\n   或者设置response.encoding=gbk。\n\n```python\nimport requests \nfrom bs4 import BeautifulSoup  \nresponse = requests.get(\'http://www.autohome.com.cn/news/\')  \nresponse.encoding=\'gbk\'\nsoup = BeautifulSoup(response.text, \'html.parser\') \nli_soup = soup.find(id=\'auto-channel-lazyload-article\').find_all(name=\'li\')\nIn [11]: for li in li_soup:\n    ...:     print(li.find(\'h3\'))\n<h3>或退出中国 铃木正与长安商议解除合资</h3>\n<h3>均降0.80万元 瑞迈柴油版全系车型调价</h3>\n<h3>起售价降至44.38万 国产普拉多全系官降</h3>\nNone\n<h3>从商用车切入 大众/福特将组成战略联盟</h3>\n<h3>奥迪S3小心了 AMG A 35 L运动轿车谍照</h3>\n<h3>6月25日亮相/三季度上市 吉利缤瑞消息</h3>\n<h3>百人口碑评新车：中华V6大气外观得好评</h3>\n<h3>汽车之家电动汽车挑战赛开场哨即将吹响</h3>\n…………………………\n```\n\n记住这里是可能存在None的，因为实际的页面中，可能穿插着各种广告什么的，这些都是正常的，所以对于这种为None的就可以忽略了，因此在循环中可以做一个判断`if not li`然后直接continue就行了。\n\n我们去到的列表中的元素每一个都是一个bs4.element.Tag的对象，因此如果要直接取内容的话直接用li.find(\'h3\').text就行了。\n\n找标签属性：\n\n```python\n# 得到一个属性的字典\nIn [15]: for li in li_soup:\n    ...:     title = li.find(\'a\')\n    ...:     if not title:\n    ...:         continue\n    ...:     print(title.attrs)\n    ...:     \n{\'href\': \'//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624\'}\n{\'href\': \'//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624\'}\n{\'href\': \'//www.autohome.com.cn/news/201806/918828.html#pvareaid=102624\'}\n{\'href\': \'//www.autohome.com.cn/news/201806/918827.html#pvareaid=102624\'}\n\n##### 一样的效果\nIn [17]: for li in li_soup:\n    ...:     title = li.find(\'a\')\n    ...:     if not title:\n    ...:         continue\n    ...:     print(title.attrs[\'href\'])\n    ...:     \n    ...:     \n//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624\n//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624\n\n# 或者使用get\nIn [16]: for li in li_soup:\n    ...:     title = li.find(\'a\')\n    ...:     if not title:\n    ...:         continue\n    ...:     print(title.get(\'href\'))\n    ...:     \n    ...:     \n//www.autohome.com.cn/news/201806/918831.html#pvareaid=102624\n//www.autohome.com.cn/news/201806/918822.html#pvareaid=102624\n//www.autohome.com.cn/news/201806/918828.html#pvareaid=102624\n//www.autohome.com.cn/news/201806/918827.html#pvareaid=102624\n//www.autohome.com.cn/news/201806/918821.html#pvareaid=102624\n```\n\n找图片：\n\n```python\nIn [18]: for li in li_soup:\n    ...:     title = li.find(\'img\')\n    ...:     if not title:\n    ...:         continue\n    ...:     print(title.attrs[\'src\'])\n    ...:     \n    ...:     \n    ...:     \n//www2.autoimg.cn/newsdfs/g26/M03/39/41/120x90_0_autohomecar__ChcCP1spq8KARebOAAGJFNHdNlQ952.jpg\n//www2.autoimg.cn/newsdfs/g26/M02/39/0D/120x90_0_autohomecar__wKgHEVspsPaASndQAAFOtCp6URI106.jpg\n```\n\n如果要把图片下载下来呢？\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nresponse = requests.get(\'http://www.autohome.com.cn/news/\')\nresponse.encoding=\'gbk\'\nsoup = BeautifulSoup(response.text, \'html.parser\')\nli_soup = soup.find(id=\'auto-channel-lazyload-article\').find_all(name=\'li\')\n\nfor li in li_soup:\n\n    img = li.find(\'img\')\n    if not img:\n        continue\n    url = img.attrs[\'src\']\n    file_name = url.rsplit(\'/\')[8]\n    url = \'http:%s\' % url\n    res = requests.get(url)\n    with open(\'/Users/lamber/tmp/%s\' % file_name, \'wb\') as f:\n        # content就是字节类型，text是文本类型\n        f.write(res.content)\n```\n\n## 自动登录github的示例\n\n1. 登录页面发送请求，获取csrftoken\n2. 发送post请求，携带用户名密码，csrf token。登录成功以后返回的内容一定是有cookie的。\n\n```python\n# https://github.com/login\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 获取token\nr1 = requests.get(\'https://github.com/login\')\ns1 = BeautifulSoup(r1.text, \'html.parser\')\ntoken = s1.find(\'input\', attrs={\'name\': \'authenticity_token\'}).get(\'value\')\n\n\n# 以post形式发送用户名密码以及csrf_token\n\"\"\"\n其实每个网站要向后台提交的数据是不一样的，如果不知道就可以去模拟测试一下，打开firebug，点击发送以后\n查看网络请求到底发了什么，github在提交的时候需要以下这些参数\n- utf8\n- authenticity_token\n- login: 标签的name等于login\n- password：这里也是标签的name属性。\n- commit: Sign in\n\"\"\"\nr2 = requests.post(\n    \'https://github.com/session\',\n    data={\n        \'utf8\': \'✓\',\n        \'authenticity_token\': token,\n        \'login\': \'1020561033@qq.com\',\n        \'password\': \'13082171785\',\n        \'commit\': \'Sign in\'\n})\n# r2.cookies这是一个对象，如果要拿到字典类型的cookie，可以继续调用get_dict()方法\ncookies_dict = r2.cookies.get_dict()\n\nr3 = requests.get(\n    url=\'https://github.com/settings/profile\',\n    cookies=cookies_dict\n)\nprint(r3.text)\n```\n\n","timestamp":1531798405220}]