if(typeof AWPageMounts=='undefined'){AWPageMounts={}};AWPageMounts['m02']=[{"name":"01-Docker介绍.md","path":"02-容器云/01-Docker基础使用/01-Docker介绍.md","content":"# Docker\n\n> Docker是Docker.Inc公司开源的一个基于LXC技术之上构建的Container容器引擎，源代码托管在github上，基于Go语言并遵循Apache2.0协议开源。\n>\n> Docker是通过内核虚拟化技术（namespaces以及cgroups等）来提供容器的资源隔离与安全保证等。由于Docker通过系统层的虚拟化实现隔离，所以Docker容器在运行的时候不需要类似虚拟机额外的操作系统的开销（经验来看一般一个完整的系统大概会消耗一个物理主机6%左右的性能），提高资源利用率。\n\n**docker的核心理念**\n\n- 构建（Build）\n- 运输（Ship）\n- 运行（Run）\n\n## Docker介绍\n\n### 容器VS虚拟化\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-11/23003726.jpg)\n\n虚拟机是完整的资源隔离。容器是隔离不是虚拟，它不需要虚拟出来操作系统环境。\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-11/3645079.jpg)\n\n### Docker可以做什么\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-11/87167288.jpg)\n\n1. 简化配置\n2. 代码流水线的管理，让开发，测试生产都跑在同样一个环境，保证开发环境的一致性。\n3. 提高开发效率\n4. 应用的隔离（虚拟机也可以做到）\n5. 整合服务器，通过docker的隔离能力整合多个服务器降低成本。\n6. 调试能力\n7. 多租户环境\n8. 快速部署\n\n### Docker改变了什么？\n\n- 面向产品：产品交付\n- 面向开发：简化环境配置\n- 面向测试：多版本测试\n- 面向运维：环境一致性，可以环境和代码一起发布，回滚的时候也可以做到一起回滚。\n- 面向架构：自动化扩容（微服务）\n\n## Docker的安装\n\n> 最新的安装信息请以官方文档的安装信息为准，[官方Doc文档地址](https://docs.docker.com/install/linux/docker-ce/centos/#set-up-the-repository)\n\n以下是一个简单的安装步骤：\n\n```shell\nyum install -y yum-utils device-mapper-persistent-data lvm2\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install docker-ce\nsystemctl start docker\n```\n\n查看docker启动状态：\n\n```shell\n[root@localhost ~]# ps -ef | grep docker\nroot     31551     1  2 00:43 ?        00:00:00 /usr/bin/dockerd\nroot     31555 31551  0 00:43 ?        00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml\nroot     31667 31082  0 00:43 pts/0    00:00:00 grep --color=auto docker\n```\n\n拉取docker镜像，这里如果直接拉的话可能会因为网速慢导致拉不下来，因此我们可以配置一下国内的docker镜像源\n\n### 配置国内docker镜像源\n\n> 国内的Docker镜像源有很多可以选择，[参考原文](https://www.cnblogs.com/anliven/p/6218741.html)\n>\n> - [DaoCloud - Docker加速器](https://dashboard.daocloud.io/)\n> - [阿里云 - 开发者平台](https://dev.aliyun.com/)\n> - [微镜像 - 希云cSphere](https://csphere.cn/hub)\n> - [镜像广场 - 时速云](https://hub.tenxcloud.com/)\n> - [灵雀云](https://hub.alauda.cn/)\n> - [网易蜂巢](https://c.163.com/)\n\n这里以阿里云的Docker加速器为例，注册并登陆[阿里云 - 开发者平台](https://dev.aliyun.com/)之后，在首页点击“创建我的容器镜像”，然后就会来到阿里云的服务面板。点击加速器标签。根据提示输入Docker登录时需要使用的密码（后期可更改），用户名就是登录阿里云的用户名。在出现的页面中，可以得到一个专属的镜像加速地址，类似于“[https://1234abcd.mirror.aliyuncs.com](https://1234abcd.mirror.aliyuncs.com/)”。根据页面中的“操作文档”信息，配置自己的Docker加速器。\n\n或者，登录[阿里云 - 容器Hub服务控制台](https://cr.console.aliyun.com/)之后，点击加速器标签，也会出现相应信息。\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-11/49585375.jpg)\n\n**其他国内Docker镜像的配置方法**\n\n国内其他Docker加速器配置方法和阿里云的差不多：\n\n- 注册账号，获取专属的镜像加速地址\n- 根据提示和系统类型，升级，并配置Docker，然后重启。\n- 验证操作结果\n\n**手动配置Docker加速器**\n\n配置Docker加速器的本质就是把Docker配置文件中的镜像下载地址由默认的Docker Hub地址变为国内镜像的加速地址。\n\n```shell\n/lib/systemd/system/docker.service\n/etc/systemd/system/docker.service\n\n# 比如centos7中\n将 OPTIONS=--registry-mirror=http://abcd1234.m.daocloud.io加入到docker的配置文件/etc/sysconfig/docker中，然后重启Docker\n```\n\n## Docker镜像\n\n### 镜像搜索\n\n```shell\n# docker有一个docker hub仓库，这个其实和github很相似。仓库中存了很多已经构建好的镜像。\n# 大多数情况我们可以直接下载一个现成的镜像而不需要我们自己构建\nCOMAND:\n\n$ sudo docker search TERM\n\nOPTIONS:\n\n--automated=false     是否仅显示自动创建的镜像  \n--no-trunc=false      是否截断输出  \n-s, --stars=0         仅显示至少有x颗星的镜像  \n\n示例:\n\n[root@maxiaoyu ~]# docker search nginx\nINDEX       NAME                    DESCRIPTION               STARS     OFFICIAL   AUTOMATED\ndocker.io   docker.io/nginx    Official build of Nginx.       7127        [OK]\n```\n\n我这里截取了搜索出结果的第一行，当然如果你实际搜索的话会有很多行的内容的，其中包含了官方的以及各种自制的。\n\n- NAME：镜像名称\n\n\n- DESCRIPTION：也就是镜像的描述，\n\n\n- STARS类似于github里面的stars，表示点赞和热度。\n\n\n- OFFICIAL：指docker标准库, 由docker 官方建立. 用户建立的image则会有userid的prefix.\n\n\n- automated builds ：则是通过代码版本管理网站结合docker hub提供的接口生成的, 例如github, bitbucket, 你需要注册docker hub, 然后使用github或bitbucket的在账户链接到docker hub, 然后就可以选择在github或bitbucket里面的项目自动build docker image, 这样的话只要代码版本管理网站的项目有更新, 就会触发自动创建image.对于的image属于什么版本，下面的“[OK]”就会打在什么地方\n\n### 获取镜像\n\n获取镜像的方式有两种，一种是直接去pull我们刚才用docker search搜索到的镜像：\n\n```shell\n[root@maxiaoyu ~]# docker pull docker.io/nginx   \nUsing default tag: latest\nTrying to pull repository docker.io/library/nginx ... \nlatest: Pulling from docker.io/library/nginx\nbc95e04b23c0: Pull complete \n110767c6efff: Pull complete \nf081e0c4df75: Pull complete \nDigest: sha256:004ac1d5e791e705f12a17c80d7bb1e8f7f01aa7dca7deee6e65a03465392072\n[root@maxiaoyu ~]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\ndocker.io/nginx     latest              1e5ab59102ce        2 weeks ago         108.3 MB\ndocker.io/centos    latest              328edcd84f1b        12 weeks ago        192.5 MB\n```\n\n导入本地的docker包：\n\n```shell\ndocker load --input centos.tar\n或者\ndocker load < nginx.tar\n```\n\n我们也可以通过docker的导出功能将我们pull下来的image导出成一个tar包，生成的tar包会保存在当前的目录：\n\n```shell\ndocker save -o centos.tar centos\n```\n\n使用docker inspect去查看image的内容：\n\n```shell\ndocker inspect docker.io/nginx:latest   \n```\n\n列出当前下载的所有的镜像：\n\n```shell\n[root@localhost ~]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnginx               latest              ae513a47849c        10 days ago         109MB\ncentos              latest              e934aafc2206        4 weeks ago         199MB\n```\n\n如果我们在docker pull的时候不加额外的参数，那么下载的就是最新的，可以在tag中看到是latest的。但是我们可以指定版本，比如：\n\n```shell\n# 参考示例\ndocker pull centos:v1.0\n```\n\n### 删除镜像\n\n```shell\ndocker rmi 镜像ID，比如：\n[root@maxiaoyu ~]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\ndocker.io/nginx     latest              1e5ab59102ce        2 weeks ago         108.3 MB\ndocker.io/centos    latest              328edcd84f1b        12 weeks ago        192.5 MB\n[root@maxiaoyu ~]# docker rmi 1e5ab59102ce\nUntagged: docker.io/nginx:latest\nUntagged: docker.io/nginx@sha256:004ac1d5e791e705f12a17c80d7bb1e8f7f01aa7dca7deee6e65a03465392072\nDeleted: sha256:1e5ab59102ce46c277eda5ed77affaa4e3b06a59fe209fe0b05200606db3aa7a\nDeleted: sha256:182a54bd28aa918a440f7edd3066ea838825c3d6a08cc73858ba070dc4f27211\nDeleted: sha256:a527b2a06e67cab4b15e0fd831882f9e6a6485c24f3c56f870ac550b81938771\nDeleted: sha256:cec7521cdf36a6d4ad8f2e92e41e3ac1b6fa6e05be07fa53cc84a63503bc5700\n```\n\n实际上是按照image的id去删除的，当然如果你的image属于被其他容器引用的情况下的话是无法删除的。会收到如下的报错信息：\n\n```shell\n[root@maxiaoyu ~]# docker rmi 328edcd84f1b\nError response from daemon: conflict: unable to delete 328edcd84f1b (must be forced) - image is being used by stopped container 1284da16efeb\n```\n\n## Docker容器\n\n### 创建一个容器\n\n```shell\n[root@maxiaoyu ~]# docker run --name myfirstdocker -i -t centos /bin/bash\n[root@2ce82d7a275e /]# uname -a\nLinux 2ce82d7a275e 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n[root@2ce82d7a275e /]# ps aux\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.1  11764  1888 ?        Ss   09:40   0:00 /bin/bash\nroot        14  0.0  0.0  47436  1676 ?        R+   09:44   0:00 ps aux\n```\n\n这样就新建了一个容器，我们也随之会进入到容器的界面。docker容器的启动，即使没有把镜像pull下来，在执行如上的命令的时候依然可以执行，因为docker发现你没有这个镜像的时候会帮你把镜像pull下来。容器的主机名就是容器的container id。接下来看看如上的几个参数都代表什么意思：\n\n- --name:指定这个生成的容器的名字，当然如果不指定的话name也会自动生成，我们指定名字只不过是为了更方便的去管理我们的容器。\n- -i：允许标准输入 ，即确保容器的STDIN是开启的。可以看到，运行命令以后我们进入到了容器中，进程为PID的是/bin/bash，也就是刚才我们指定要运行的命令。因此docker其实是为进程执行隔离作用的，虚拟机是给操作系统做隔离的。\n- -t：开启一个tty伪终端。\n- -d：如果需要容器在后台运行的话，可以加上-d参数。范围值为容器的id\n\n>以上操作其实是经历了这样一个过程：\n>\n>1. 执行上面操作首先会在系统本地去找有没有对应的这样一个image，如果没有的话那么就会去Docker Hub Registry去找，一旦找到以后就回去下载然后保存到本地宿主机器。\n>2. Docker利用这个image创建了一个容器，这个容器拥有自己的网络，ip，以及桥接外部网络的接口。\n>3. 我们告诉这个容器要去执行什么命令（/bin/bash），当然这个命令是可以在容器中指定的，指定容器起来的时候自动执行某个命令，那这里就可以不写了。\n\n当前我们是在容器的内部，通过`ps aux`我们可以得知，pid的大树根并不是init（centos7的树根并不是init），而是我们运行的/bin/bash，如果此时我们使用exit退出容器的话，那么此时的容器就会被关掉，它就完成了它的使命。\n\n```shell\n[root@2ce82d7a275e /]# exit\nexit\n[root@maxiaoyu ~]# docker ps -a\nCONTAINER ID  IMAGE    COMMAND      CREATED         STATUS                   PORTS     NAMES     \n2ce82d7a275e  centos  \"/bin/bash\"  10 minutes ago  Exited (0) 5 seconds ago       myfirstdocker\n```\n\nDocker其实是单进程的，他需要一个进程在前台跑着，因此如果这个程序执行完了，那么整个容器也就退出了，就像我们刚才执行/bin/bash后进入到容器，当exit退出的时候这个进程结束以后整个容器也就跟着一起退出了。所以说不是所有的应用都适合docker。\n\n### 启动并进入一个容器\n\n那么如何启动已经关闭的容器呢？\n\n**方法一**\n\n```\ndocker start docker_name\n\n比如（这样就一直跑起来了）：\n[root@maxiaoyu ~]# docker start myfirstdocker\nmyfirstdocker\n```\n\n**方法二**\n\n```\n[root@maxiaoyu ~]# docker attach myfirstdocker\n[root@2ce82d7a275e /]# \n# 这样就是直接进到容器里面去了，不过exit以后容器还是会停止，因此一般不会去这么操作的\n# 而且多人同时进入到这个容器这个命令显示是同步的。\n```\n\n**方法三**\n\n```shell\n# 生产中建议使用的方法\n[root@maxiaoyu ~]# yum -y install util-linux\n[root@maxiaoyu ~]# docker start mydocker\nmydocker\n# 获取当前的docker的pid，如果获取到的pid=0，说明你这个容器没起来，后面可以接容器id也可以接容器名称。\n[root@maxiaoyu ~]# docker inspect -f \"{{ .State.Pid }}\" mydocker\n13500\n[root@maxiaoyu ~]# nsenter -t 13500 -m -u -i -n -p \n# 使用这种方法进入容器，即使退出的话仍然可以保证容器是开启的，进程并不会结束，docker ps能看到\n[root@1284da16efeb /]# exit\nlogout\n# 那么为什么即使exit出去也不会退出容器呢？\n[root@1284da16efeb /]# ps aux\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.0  11764  1676 ?        Ss+  10:01   0:00 /bin/bash\nroot        26  0.0  0.1  15192  1996 ?        S    10:05   0:00 -bash\nroot        39  0.0  0.0  50868  1816 ?        R+   10:05   0:00 ps aux\n# 主要原因是因为多了一个-bash的进程，因此退出当前的进程这个docker不会退出，因为仍然还有一个/bin/bash的进程正在运行，因此整个容器是并不会退出的。\n```\n\n查看一下nsenter的帮助信息：\n\n```\n[root@maxiaoyu ~]# nsenter --help\n\nUsage:\n nsenter [options] <program> [<argument>...]\n\nRun a program with namespaces of other processes.\n\nOptions:\n -t, --target <pid>     target process to get namespaces from\n -m, --mount[=<file>]   enter mount namespace\n -u, --uts[=<file>]     enter UTS namespace (hostname etc)\n -i, --ipc[=<file>]     enter System V IPC namespace\n -n, --net[=<file>]     enter network namespace\n -p, --pid[=<file>]     enter pid namespace\n -U, --user[=<file>]    enter user namespace\n -S, --setuid <uid>     set uid in entered namespace\n -G, --setgid <gid>     set gid in entered namespace\n     --preserve-credentials do not touch uids or gids\n -r, --root[=<dir>]     set the root directory\n -w, --wd[=<dir>]       set the working directory\n -F, --no-fork          do not fork before exec\'ing <program>\n -Z, --follow-context   set SELinux context according to --target PID\n\n -h, --help     display this help and exit\n -V, --version  output version information and exit\n\nFor more details see nsenter(1).\n```\n\n因此我们可以针对第三种生产中使用的方法去写一个小脚本然后通过批量部署分发后使用：\n\n```shell\n# $1可以是容器的名称，或者容器的id\n[root@maxiaoyu ~]# cat docker_in.sh \n#!/bin/bash\n\n# Use nsenter to access docker \n\ndocker_in (){\n   NAME_ID=$1\n   PID=$(docker inspect -f \"{{ .State.Pid }}\" $NAME_ID)\n   nsenter -t $PID -m -u -i -n -p\n}\n\ndocker_in $1\n[root@maxiaoyu ~]# chmod +x docker_in.sh\n```\n\n**方法四**\n\n```shell\n[root@localhost ~]# docker exec mydocker uptime\n 06:38:47 up  3:51,  0 users,  load average: 0.00, 0.03, 0.05\n```\n\n我不想真的进入容器，但是我还想让这个容器给我执行个命令，就可以使用exec，这个是exec命令的用途本意。不过通过exec也能进入容器，比如：\n\n```shell\n[root@linux-node1 ~]# docker exec -it mydocker /bin/bash\n[root@e95a62d6770f /]# ps aux\nUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot          1  0.0  0.0  11768  1676 ?        Ss+  02:28   0:00 /bin/bash\nroot         50  0.5  0.0  11768  1884 ?        Ss   02:41   0:00 /bin/bash\nroot         62  0.0  0.0  47440  1676 ?        R+   02:41   0:00 ps aux\n###有两个/bin/bash，因此这个容器技术退出了仍然还在运行，他退出的是pid=50的/bin/bash\n\n[root@e95a62d6770f /]# exit\nexit\n[root@linux-node1 ~]# docker ps -a\nCONTAINER ID    IMAGE    COMMAND     CREATED         STATUS        PORTS         NAMES\ne95a62d6770f    centos  \"/bin/bash\" 33 minutes ago  Up 12 minutes              mydocker\n78fc36ba1e5a    centos  \"/bin/echo xx\" 39 minutes ago  Exited (0) 39 minutes ago                       compassionate_rosalind\n```\n\n### 删除容器\n\n```shell\ndocker rm 容器id\n```\n\n如果容器在使用的话那是不允许你删除的，不过你可以使用-f强制干掉。当然一般不建议这么做。一般来讲都是先把容器停掉以后再把容器干掉。\n\n运行玩意后直接删除容器\n\n```shell\n## --rm参数可以使得一个容器运行完成以后就直接删除了。下面的内容执行完成以后再docker ps就看不到了\n[root@linux-node1 ~]# docker run --rm centos /bin/echo \"hello lamber\"\nhello lamber\n```\n\n### 停止容器\n\n```shell\ndocker stop 容器ID\n```\n\n\n\n","timestamp":1528078404016},{"name":"02-Docker数据存储和网络访问.md","path":"02-容器云/01-Docker基础使用/02-Docker数据存储和网络访问.md","content":"# Docker数据存储&网络访问\n\n## Docker的网络应用\n\n### 通过端口映射的方式访问Docker\n\n我们安装了docker后会多了一个桥接网卡docker0：\n\n```shell\n[root@localhost ~]# ifconfig docker0\ndocker0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255\n        inet6 fe80::42:baff:fea1:dfa8  prefixlen 64  scopeid 0x20<link>\n        ether 02:42:ba:a1:df:a8  txqueuelen 0  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 10  bytes 1876 (1.8 KiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n[root@localhost ~]# yum install bridge-utils\n\n[root@localhost ~]# brctl show\nbridge name\tbridge id\t\tSTP enabled\tinterfaces\ndocker0\t\t8000.0242baa1dfa8\tno\t\tveth92ad03f\n```\n\n那么现在启动一个容器并映射端口，端口的映射分为随机映射和指定映射：\n\n- 随机映射\n  - docker run -P\n- 指定映射\n  - -p hostPort:containerPort       # 映射所有ip的指定端口到容器ip的指定端口\n  - -p ip:hostPort:containerPort   # 如果物理机有多个ip地址，我们也可以指定一个ip\n  - -p ip::containerPort   # 映射到指定地址的任意端口\n  - -p hostPort:containerPort:udp # 默认是tcp的，可以指定协议为udp\n  - -p 80:81 -p 443:443 ##同时映射多个端口\n\n#### 随机端口映射\n\n由于之前下载了一个nginx的镜像，那么我们就直接先来启用一个nginx容器：\n\n```shell\n[root@localhost ~]# docker run -d -P --name nginx-test1 nginx\nWARNING: IPv4 forwarding is disabled. Networking will not work.\n0b3299999a96eb216b05f2a5c68f32a2775751a49d240034a2f89305bb7bd236\n\n# 可以看到容器启动，返回了容器的id的，但是报了一个警报说如果不打开ipv4转发的话那么网络将不会有效。那么我们就先把系统的网络转发打开\necho 1 > /proc/sys/net/ipv4/ip_forward\n\n# 查看容器的状态\n[root@localhost ~]# docker ps\nCONTAINER ID IMAGE   COMMAND              CREATED             STATUS              PORTS                   NAMES\n0b3299999a96 nginx \"nginx -g \'daemon of…\"   3 minutes ago       Up 3 minutes        0.0.0.0:32768->80/tcp   nginx-test1\n```\n\n从上面可以看到将nginx容器的80端口映射到了宿主机的32768端口。此时我们就可以通过访问宿主机的32768端口来访问docker了，具体我还可以通过iptables来查看，`iptables -t nat nvL`。可以通过docker logs查看日志：\n\n```shell\n[root@localhost ~]# docker logs nginx-test1\n192.168.56.1 - - [11/May/2018:07:32:23 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\" \"-\"\n2018/05/11 07:32:24 [error] 5#5: *1 open() \"/usr/share/nginx/html/favicon.ico\" failed (2: No such file or directory), client: 192.168.56.1, server: localhost, request: \"GET /favicon.ico HTTP/1.1\", host: \"192.168.56.101:32768\", referrer: \"http://192.168.56.101:32768/\"\n192.168.56.1 - - [11/May/2018:07:32:24 +0000] \"GET /favicon.ico HTTP/1.1\" 404 572 \"http://192.168.56.101:32768/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\" \"-\"\n```\n\n随机映射是非常简单的，但是管理起来也不是很方便的，因为我们需要知道每个容器的端口的映射关系，好处就是端口不会重复，启用很方便。\n\n#### 指定映射\n\n```shell\n[root@localhost ~]# docker run -d -p 192.168.56.101:88:80 --name nginx-test2 nginx\n\nf2eccbad48a94111e7844562ce8750584bced374894cd4dad0af7b201eef8ffc\n[root@localhost ~]# curl 192.168.56.101:88 -I\nHTTP/1.1 200 OK\nServer: nginx/1.13.12\nDate: Fri, 11 May 2018 07:45:34 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Mon, 09 Apr 2018 16:01:09 GMT\nConnection: keep-alive\nETag: \"5acb8e45-264\"\nAccept-Ranges: bytes\n```\n\n查看容器映射：\n\n```shell\n# -l参数指的是查看最后一个最新创建的容器\n[root@localhost ~]# docker ps -l\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                       NAMES\nf2eccbad48a9        nginx               \"nginx -g \'daemon of…\"   2 minutes ago       Up 2 minutes        192.168.56.101:88->80/tcp   nginx-test2\n```\n\n查看一下iptables的nat规则：\n\n```shell\nChain DOCKER (2 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 RETURN     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0           \n    2   128 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:32768 to:172.17.0.3:80\n    1    60 DNAT       tcp  --  !docker0 *       0.0.0.0/0            192.168.56.101       tcp dpt:88 to:172.17.0.4:80\n```\n\n经过NAT转换所以对于吞吐量比较大的业务还是多少有一些影响的。\n\n## Docker数据存储\n\n> 生产中不建议直接在docker容器中去写数据，建议使用数据卷或者数据卷容器写入\n\n### 数据卷\n\n我们新建一个容器，并把物理机的一个区域给它mount到容器中的data目录下，使用-v参数：\n\n```shell\n[root@localhost ~]# docker run -d --name nginx-volume-test1 -v /data nginx\n7cb33525f1027208402a86c95d0deedc4bf592264d113ddb47901d46797daa94\n```\n\n为了方便测试，我们直接去容器内的data目录下新建了一个hehe文件。那么这个文件被映射到物理机的什么地方呢？\n\n```shell\n[root@localhost ~]# docker inspect -f {{.Mounts}} nginx-volume-test1\n[{volume c2316666b28d0875aa66ec90ef511e61c0e044e531d56a54e835789ac9792229 /var/lib/docker/volumes/c2316666b28d0875aa66ec90ef511e61c0e044e531d56a54e835789ac9792229/_data /data local  true }]\n[root@localhost ~]# cd /var/lib/docker/volumes/c2316666b28d0875aa66ec90ef511e61c0e044e531d56a54e835789ac9792229/_data\n[root@localhost _data]# ll\n总用量 0\n-rw-r--r--. 1 root root 0 5月  11 04:14 hehe\n```\n\n另外一种挂载方式，指定我们的映射目录，一个源一个目的：\n\n```shell\n# 源是物理机的，目的是docker容器的。注意对应关系\nmkdir /data/docker-volume-nginx -p\ndocker run -d --name nginx-volume-test2 -v /data/docker-volume-nginx/:/data nginx\n6b739fc4d20f1064abf5c2fb619a394772f445f15636b303b0527bb148bd2818\n\ntouch /data/docker-volume-nginx/hehehe\ndocker_in.sh nginx-volume-test2\n# 进入容器\nroot@6b739fc4d20f:/# ls -l /data/\ntotal 0\n-rw-r--r-- 1 root root 0 Mar 31 08:20 hehehe\n```\n\n但是上面的这种方法在dockerfile中就是不支持的，试想如果这样操作了，那么可移植性就下降了，你要确保移植的位置也有你这个物理机的目录和数据。\n\n其他的几个选项：\n\n```shell\ndocker run -d --name nginx-volume-test2 -v /data/docker-volume-nginx/:/data:ro  只读\n```\n\n挂载文件：\n\n```shell\n[root@linux-node1 ~]# docker run --rm -it -v /root/docker_in.sh:/root/ nginx /bin/bash\nroot@239820f3e6bd:/# ls /root/ -l --color\ntotal 4\n-rwxr-xr-x 1 root root 179 Mar 31 07:48 docker_in.sh\n\n###记得前后文件名要对应\n```\n\n### 数据卷容器\n\n实现数据在多个容器之间共享\n\n```shell\n[root@linux-node1 ~]# docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                               NAMES\n6b739fc4d20f        nginx               \"nginx -g \'daemon off\"   11 minutes ago      Up 11 minutes       80/tcp, 443/tcp                     nginx-volume-test2\n925564d95872        nginx               \"nginx -g \'daemon off\"   20 minutes ago      Up 20 minutes       80/tcp, 443/tcp                     nginx-volumn-test1\ndce2e78b22be        nginx               \"nginx -g \'daemon off\"   36 minutes ago      Up 36 minutes       443/tcp, 192.168.56.11:81->80/tcp   nginx2\n[root@linux-node1 ~]# docker run -it --name volume-test3 --volumes-from nginx-volume-test2 centos /bin/bash\n[root@034018de2dc4 /]# ls /data/\nhehehe\n```\n\n即使把nginx-volume-test2给停掉了仍然不影响volume-test3的访问，因为这个数据卷是mount上去的。而且我们这个时候占用的这个共享卷的时候，这个nginx-volume-test2你是删除不掉的。\n\n**实际应用**\n\n比如创建一个容器挂载数据卷，然后其他容器都共享它的。比如可以用到日志统一管理，多个容器挂载一个日志数据卷，然后用logstash统一收集管理。\n\n```shell\n[root@linux-node1 ~]# docker run -d --name nfs-test -v /data/nfs-data/:/data nginx\n[root@linux-node1 ~]# docker run --rm -it --volumes-from nfs-test centos /bin/bash\n[root@ac637a64910f /]# cd /data/\n[root@ac637a64910f data]# ls -l\ntotal 0\n-rw-r--r-- 1 root root 0 Mar 31 08:41 iamatest\n此时我们在物理机上创建的测试文件就显示出来了。\n```\n\n","timestamp":1528078404016},{"name":"03-手动构建镜像.md","path":"02-容器云/01-Docker基础使用/03-手动构建镜像.md","content":"# Docker手动构建\n\n杀死所有正在运行的容器并删除它\n\n```shell\n[root@linux-node1 ~]# docker ps -a -q\n319cc917e421\n00c801224fed\n9af1e1593e1c\n034018de2dc4\n6b739fc4d20f\n925564d95872\ndce2e78b22be\n35e9b3386c8a\ndfff10104bae\ne95a62d6770f\n78fc36ba1e5a\n[root@linux-node1 ~]# docker kill $(docker ps -a -q)\n[root@linux-node1 ~]# docker rm $(docker ps -a -q)\n```\n\n然后我们重新创建一个新的centos的容器，安装nginx：\n\n```shell\n[root@linux-node1 ~]# docker run --name mynginx -it centos \n\n# 进入容器，安装扩展源\n[root@c7426bc61497 /]# rpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm\n\n# 安装nginx，这里测试用就直接yum了，生产中一般都是编译安装：\n[root@c7426bc61497 /]# yum install -y nginx\n\n# 前面说到容器的运行需要前台进程的支撑，因此如果nginx在后台跑的话这个docker容器是起不来的，因此我们需要对nginx做一些修改，让它在前台运行.\n[root@c7426bc61497 /]# grep -v \'^#\' /etc/nginx/nginx.conf | head -1\ndaemon off;\n配置文件中添加一行deamon off，然后退出容器\n[root@linux-node1 ~]# docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES\nc7426bc61497        centos              \"/bin/bash\"         12 minutes ago      Exited (0) 4 seconds ago                       mynginx\n```\n\n对容器修改完以后接下来我们需要提交，其实这个操作很类似于Git，其中-m指的是comment，就是一个描述，后面跟container id，然后nginx是库名称。\n\n```shell\n[root@linux-node1 ~]# docker commit -m \"My nginx\" c7426bc61497 nginx/mynginx:v1\nsha256:81f1607bb8a07d4874a68e1cffa369707f243e935f381373d8482cde6ee6a6ed\n[root@linux-node1 ~]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\nnginx/mynginx       v1                  81f1607bb8a0        About a minute ago   355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago           182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago          192.5 MB\n```\n\n运行测试：\n\n```shell\n[root@linux-node1 ~]# docker run --name mynginxv1 -d -p 81:80 nginx/mynginx:v1 nginx\n这个名称要写全了，如果不写全的话，它会使用别的版本的最后这一个nginx是执行的命令。\n```\n\n![](http://omk1n04i8.bkt.clouddn.com/17-3-31/84755152-file_1490951666292_e7.jpg)\n\n","timestamp":1528078404016},{"name":"04-DockerFile.md","path":"02-容器云/01-Docker基础使用/04-DockerFile.md","content":"# DockerFile\n\n> Dockerfile是为快速构建docker image而设计的，当你使用dockerbuild 命令的时候，docker 会读取当前目录下的命名为Dockerfile(首字母大写)的纯文本文件并执行里面的指令构建出一个docker image。这比SaltStack的配置管理要简单的多，不过还是要掌握一些简单的指令。 Dockerfile 由一行行命令语句组成，并且支持以#开头的注释行。指令是不区分大小写的，但是通常我们都大写。\n\n下面我们通过构建一个Nginx的镜像来学习Dockerfile. 首先实现创建好目录，我这在opt下创建目录dockerfile，然后在dockerfile目录下创建nginx目录，在nginx目录下新建一个Dockerfile文件。Dcockerfile这个文件的首字母要大写，不然有可能不会被识别。\n\n```shell\n[root@linux-node1 ~]# cd /opt/\n[root@linux-node1 opt]# cd dockerfile/\n[root@linux-node1 dockerfile]# cd nginx/\n[root@linux-node1 nginx]# echo \"nginx in Docker ,hahahah\" >index.html\n[root@linux-node1 nginx]# cat Dockerfile \n# This Dockerfile is used to practice \n# Version: 1.0\n# Author: lamber\n\n# Base Image\nFROM centos\n\n# Who will take care of this image\nMAINTAINER lamber 1020561033@qq.com\n\n# Prepare Epel\nRUN rpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm\n\n# Install Nginx and deal with the config file\nRUN yum -y install nginx && yum clean all\nRUN echo \"daemon off;\" >> /etc/nginx/nginx.conf\nADD index.html /usr/share/nginx/html/index.html\n\n# Run\nEXPOSE 80\nCMD [\"nginx\"]\n```\n\n说明：\n\n- FROM：这个镜像的妈是谁，指定基础镜像，除了注释外的第一行必须是他。如果本地没有这个镜像，它会给你pull下来。\n- MAINTAINER：告诉别人谁负责养他，指定维护者的信息\n- RUN：你想让他干啥，在命令前面加上RUN就可以了。\n- ADD：给它点创业资金，copy文件进去，会自动解压\n- WORKDIR：我是cd，今天刚化了妆（设置当前的工作目录）\n- VOLUME：给它一个存放行李的地方，设置卷，挂载主机目录\n- EXPOSE：它要打开的门是啥，指定对外的端口\n- CMD：奔跑吧，兄弟，指定容器启动后要干的事情，这里双引一下，单引可能出现问题。。。。\n\n构建docker镜像（后面那个点就是在当前目录，意思是告诉docker去哪里找这个dockerfile，一个点就是在当前目录找dockerfile，这个你也可以写绝对路径也是ok的），然后创建容器：\n\n```shell\n[root@linux-node1 nginx]# docker build -t mynginx:v2 .\n[root@linux-node1 nginx]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nmynginx             v2                  5104f2ed9887        5 seconds ago       280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        32 minutes ago      355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n[root@linux-node1 nginx]# docker run --name mynginxv2 -d -p 82:80 mynginx:v2 nginx\na1efc632ba6d2412c3bc9fded592ca289c0f6c14dd1832118fc52539a2c1123f\n```\n\n起来以后我们就可以在网页端进行测试了！\n\n## 附录：Dockerfile指令详解\n\n下面我们来分别介绍下上面使用到的命令：\n\n#### FROM\n\n- 格式：FROM `<image>` 或FROM `<image>:<tag>`。\n- 解释：FROM是Dockerfile里的第一条指令（必须是），后面跟有效的镜像名（如果该镜像你的本地仓库没有则会从远程仓库Pull取）。然后后面的其它指令FROM的镜像中执行。\n\n#### MAINTAINER\n\n- 格式：MAINTAINER  `<name>`\n- 解释：指定维护者信息。\n\n#### RUN\n\n格式：RUN `<command>` 或 RUN [\"executable\", \"param1\", \"param2\"]。 解释：运行命令，命令较长使可以使用\\来换行。推荐使用上面数组的格式\n\n#### CMD\n\n```\n格式：\n   CMD [\"executable\",\"param1\",\"param2\"] 使用 exec 执行，推荐方式；\n   CMD command param1 param2 在 /bin/sh 中执行，提供给需要交互的应用；\n   CMD [\"param1\",\"param2\"] 提供给ENTRYPOINT的默认参数；\n```\n\n解释： CMD指定容器启动是执行的命令，每个Dockerfile只能有一条CMD命令，如果指定了多条，只有最后一条会被执行。如果你在启动容器的时候也指定的命令，那么会覆盖Dockerfile构建的镜像里面的CMD命令。\n\n#### ENTRYPOINT\n\n```\n格式：\n   ENTRYPOINT [\"executable\", \"param1\",\"param2\"]\n   ENTRYPOINT command param1 param2（shell中执行）。\n```\n\n解释：和CMD类似都是配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。 每个 Dockerfile 中只能有一个ENTRYPOINT，当指定多个时，只有最后一个起效。ENTRYPOINT没有CMD的可替换特性，也就是你启动容器的时候增加运行的命令不会覆盖ENTRYPOINT指定的命令。\n\n所以生产实践中我们可以同时使用ENTRYPOINT和CMD，例如：\n\n```\nENTRYPOINT [\"/usr/bin/rethinkdb\"]\nCMD [\"--help\"]\n```\n\n#### USER\n\n- 格式：USER daemon\n- 解释：指定运行容器时的用户名和UID，后续的RUN指令也会使用这里指定的用户。\n\n#### EXPOSE\n\n- 格式：EXPOSE `<port> [<port>…]`\n- 解释：设置Docker容器内部暴露的端口号，如果需要外部访问，还需要启动容器时增加-p或者-P参数进行分配。\n\n#### ENV\n\n```\n格式：\nENV <key> <value>\nENV <key>=<value> ...\n```\n\n解释：设置环境变量，可以在RUN之前使用，然后RUN命令时调用，容器启动时这些环境变量都会被指定\n\n#### ADD\n\n```\n格式：\n   ADD <src>... <dest>\n   ADD [\"<src>\",... \"<dest>\"]\n```\n\n解释：将指定的<src>复制到容器文件系统中的<dest> 所有拷贝到container中的文件和文件夹权限为0755,uid和gid为0 如果文件是可识别的压缩格式，则docker会帮忙解压缩\n\n#### VOLUME\n\n- 格式：VOLUME [\"/data\"]\n- 解释：可以将本地文件夹或者其他container的文件夹挂载到container中。\n\n#### WORKDIR\n\n- 格式：WORKDIR/path/to/workdir\n- 解释：切换目录，为后续的RUN、CMD、ENTRYPOINT 指令配置工作目录。\n\n可以多次切换(相当于cd命令)， 也可以使用多个WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。\n\n```\n例如\nWORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n则最终路径为 /a/b/c。\n```\n\n#### ONBUILD\n\nONBUILD 指定的命令在构建镜像时并不执行，而是在它的子镜像中执行\n\n#### ARG\n\n- 格式：ARG `<name>[=<default value>]`\n- 解释：ARG指定了一个变量在docker build的时候使用，可以使用--build-arg  `<varname>=<value>`来指定参数的值，不过如果构建的时候不指定就会报错。\n\n参考自运维社区：https://mp.weixin.qq.com/s/A8KSp3zmpL_u9a2aYv8WYw","timestamp":1528078404016},{"name":"01-Docker镜像生产实战.md","path":"02-容器云/02-Docker进阶/01-Docker镜像生产实战.md","content":"# Docker镜像生产规划与实战\n\n> 公开的dockerfile：https://github.com/dockerfile\n\n## 分层的设计\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-15/1214172.jpg)\n\n```\nA(系统层)-->B(运行环境层)-->C(应用服务层)\n```\n\n\n\n```\n[root@linux-node1 ~]# mkdir docker\n[root@linux-node1 ~]# cd docker/\n[root@linux-node1 docker]# mkdir system\n[root@linux-node1 docker]# mkdir runtime\n[root@linux-node1 docker]# mkdir app\n[root@linux-node1 docker]# cd system/\n[root@linux-node1 system]# mkdir centos\n[root@linux-node1 system]# mkdir ubuntu\n[root@linux-node1 system]# mkdir centos-ssh\n[root@linux-node1 system]# cd ../runtime/\n[root@linux-node1 runtime]# mkdir php\n[root@linux-node1 runtime]# mkdir java\n[root@linux-node1 runtime]# mkdir python\n[root@linux-node1 runtime]# cd ../app/\n[root@linux-node1 app]# mkdir xxx.api\n[root@linux-node1 app]# mkdir xxx.admin\n[root@linux-node1 docker]# tree .\n.\n├── app\n│   ├── xxx.admin\n│   └── xxx.api\n├── runtime\n│   ├── java\n│   ├── php\n│   └── python\n└── system\n    ├── centos\n    ├── centos-ssh\n    └── ubuntu\n\n```\n## 模拟案例\n### 规划：\nrequirement.txt\n\npip install -r requirement.txt\n\n创建dockerfile然后build镜像\n\n### 构建一个基础的centos镜像\n```\n[root@linux-node1 docker]# cd system/centos\n[root@linux-node1 centos]# pwd\n/root/docker/system/centos\n[root@linux-node1 centos]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\n--2017-03-31 18:13:42--  http://mirrors.aliyun.com/repo/epel-7.repo\n正在解析主机 mirrors.aliyun.com (mirrors.aliyun.com)... 115.28.122.210, 112.124.140.210\n正在连接 mirrors.aliyun.com (mirrors.aliyun.com)|115.28.122.210|:80... 已连接。\n已发出 HTTP 请求，正在等待回应... 200 OK\n长度：1084 (1.1K) [application/octet-stream]\n正在保存至: “/etc/yum.repos.d/epel.repo”\n\n100%[====================================================================================>] 1,084       --.-K/s 用时 0s      \n\n2017-03-31 18:13:42 (16.7 MB/s) - 已保存 “/etc/yum.repos.d/epel.repo” [1084/1084])\n\n[root@linux-node1 centos]# cp /etc/yum.repos.d/epel.repo .\n\n```\n\n\n```\n[root@linux-node1 centos]# vim Dockerfile\n# Docker for CentOS\n\nFROM centos\n\nMAINTAINER lamber 1020561033@qq.com\n\n# EPEL\nADD epel.repo /etc/yum.repos.d/\n\n# Base pkg\nRUN yum install -y wget mysql-devel supervisor git redis tree net-tools sudo psmisc && yum clean all\n\n[root@linux-node1 centos]# docker build -t lamber/centos:base .\n[root@linux-node1 centos]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nlamber/centos       base                6a427d784875        6 seconds ago       283.1 MB\nmynginx             v2                  5104f2ed9887        42 minutes ago      280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        About an hour ago   355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n\n```\n再创建运行环境的时候就应该from上面我们创建的镜像了：\n\n```\n[root@linux-node1 python]# cat Dockerfile \nFROM lamber/centos:base\n\nMAINTAINER lamber 1020561033@qq.com\n\n# Python env\nRUN yum install -y python-devel python-pip supervisor\n\n# Upgrade pip\nRUN pip install --upgrade pip\n\n\n```\n这里引用的就是上面我们创建好的镜像。\n\n构建docker镜像\n\n```\n[root@linux-node1 python]# docker build -t lamber/python .\n[root@linux-node1 app]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nlamber/python       latest              94747e30d7b4        2 minutes ago       413.3 MB\nlamber/centos       base                6a427d784875        About an hour ago   283.1 MB\nmynginx             v2                  5104f2ed9887        2 hours ago         280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        2 hours ago         355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n\n\n```\n## 构建一个带ssh的centos docker镜像\n\n```\n[root@linux-node1 centos-ssh]# pwd\n/root/docker/system/centos-ssh\n[root@linux-node1 centos-ssh]# cat Dockerfile \n# Docker for CentOS\n\nFROM centos\n\nMAINTAINER lamber 1020561033@qq.com\n\n# EPEL\nADD epel.repo /etc/yum.repos.d/\n\n# Base pkg\nRUN yum install -y openssh-clients openssl-devel openssh-server wget mysql-devel supervisor git redis tree net-tools sudo psmisc && yum clean all\n\n# For SSHD\nRUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key\nRUN echo \"root:redhat\" | chpasswd\n[root@linux-node1 centos-ssh]# docker build -t lamber/centos-ssh .\n[root@linux-node1 centos-ssh]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nlamber/centos-ssh   latest              41db0593e915        2 minutes ago       284.1 MB\nlamber/python       latest              94747e30d7b4        16 minutes ago      413.3 MB\nlamber/centos       base                6a427d784875        About an hour ago   283.1 MB\nmynginx             v2                  5104f2ed9887        2 hours ago         280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        3 hours ago         355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n\n```\n部署环境层：\n\n```\n[root@linux-node1 runtime]# cp -r python/ python-ssh\n[root@linux-node1 runtime]# ll\n总用量 0\ndrwxr-xr-x 2 root root  6 3月  31 18:08 java\ndrwxr-xr-x 2 root root  6 3月  31 18:08 php\ndrwxr-xr-x 2 root root 24 3月  31 19:52 python\ndrwxr-xr-x 2 root root 24 3月  31 22:39 python-ssh\n[root@linux-node1 runtime]# cd python\n[root@linux-node1 python]# cat Dockerfile \nFROM lamber/centos-ssh\n\nMAINTAINER lamber 1020561033@qq.com\n\n# Python env\nRUN yum install -y python-devel python-pip supervisor\n\n# Upgrade pip\nRUN pip install --upgrade pip\n[root@linux-node1 python]# docker build -t lamber/python-ssh .\n[root@linux-node1 python]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nlamber/python-ssh   latest              657b57f25bdd        24 seconds ago      414.3 MB\nlamber/centos-ssh   latest              41db0593e915        2 hours ago         284.1 MB\nlamber/python       latest              94747e30d7b4        2 hours ago         413.3 MB\nlamber/centos       base                6a427d784875        4 hours ago         283.1 MB\nmynginx             v2                  5104f2ed9887        5 hours ago         280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        5 hours ago         355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n\n```\n部署应用层：\n\n测试脚本：\n\n```\n[root@linux-node1 shop-api]# pwd\n/root/docker/app/shop-api\n\n[root@linux-node1 shop-api]# cat app.py \nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef hello():\n    return \'Hello World!\' \n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", debug=True)\n\n```\n测试脚本在本机进行测试（要首先确保本地跑没问题，然后再封装进docker镜像里面去）：\n\n```\n[root@linux-node1 shop-api]# yum -y install python-pip\n[root@linux-node1 python-ssh]# pip install flask\n[root@linux-node1 shop-api]# python app.py \n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n * Restarting with stat\n * Debugger is active!\n * Debugger PIN: 248-731-527\n\n起到了5000端口，访问测试\n[root@linux-node1 shop-api]# curl  127.0.0.1:5000 \nHello World!\n```\n创建Dockerfile：\n\n```\n[root@linux-node1 shop-api]# cat Dockerfile \n#Base image\nFROM lamber/python-ssh\n\n#Maintainer\nMAINTAINER lamber 1020561033@qq.com\n\n# Add www user\nRUN useradd -s /sbin/nologin -M www\n\n# ADD file\nADD app.py /opt/app.py\nADD requirements.txt /opt/\nADD supervisord.conf /etc/supervisord.conf\nADD app-supervisor.ini /etc/supervisord.d/\n\n# pip\nRUN /usr/bin/pip2.7 install -r /opt/requirements.txt\n\n# Port\nEXPOSE 22 5000\n\n#CMD\nCMD [\"/usr/bin/supervisord\", \"-c\", \"/etc/supervisord.conf\"]\n##有参数是上面这种写法\n```\n\n创建supervisor的配置文件：\n\n```\n[root@linux-node1 shop-api]# cat app-supervisor.ini \n[program:shop-api]\ncommand=/usr/bin/python2.7 /opt/app.py\nprocess_name=%(program_name)s\nautostart=true\nuser=www\nstdout_logfile=/tmp/app.log\nstderr_logfile=/tmp/app.error\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\nprocess_name=%(program_name)s\nautostart=true\n\n```\n\n创建依赖文件：\n\n```\n[root@linux-node1 shop-api]# cat requirements.txt \nflask\n\n```\n修改supervisor.conf的配置文件：\n\n```\n[root@linux-node1 shop-api]# cat supervisord.conf | grep nodaemon\nnodaemon=true              ; (start in foreground if true;default false)\n将nodaemon改为true，这样supervisor就在前台运行了，如此创建容器以后就能起来了。\n```\n\n补全文件：\n\n```\n[root@linux-node1 shop-api]# ll\n总用量 24\n-rw-r--r-- 1 root root  172 9月  10 2016 app.py\n-rw-r--r-- 1 root root  257 9月  10 2016 app-supervisor.ini\n-rw-r--r-- 1 root root  433 9月  10 2016 Dockerfile\n-rw-r--r-- 1 root root    6 9月  10 2016 requirements.txt\n-rw-r--r-- 1 root root 7952 9月  10 2016 supervisord.conf\n```\n生成docker镜像：\n\n```\n[root@linux-node1 shop-api]# docker build -t lamber/shop-api .\n[root@linux-node1 shop-api]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nlamber/shop-api     latest              9a9470aa92eb        32 seconds ago      420.3 MB\nlamber/python-ssh   latest              f1c4bcf54bad        19 minutes ago      414.3 MB\nlamber/python       latest              570e81c8e8bf        23 minutes ago      413.3 MB\nlamber/centos-ssh   latest              41db0593e915        3 hours ago         284.1 MB\nlamber/centos       base                6a427d784875        4 hours ago         283.1 MB\nmynginx             v2                  5104f2ed9887        5 hours ago         280.7 MB\nnginx/mynginx       v1                  81f1607bb8a0        6 hours ago         355 MB\ndocker.io/nginx     latest              5e69fe4b3c31        3 days ago          182.5 MB\ndocker.io/centos    latest              98d35105a391        2 weeks ago         192.5 MB\n\n```\n启动镜像容器：\n\n```\n[root@linux-node1 shop-api]# docker run --name shop-api -d -p 88:5000 -p 8022:22 lamber/shop-api\nf38807176f44970632f0156bd02a614c102343749a627a1eb945a6e745c2e95b\n[root@linux-node1 shop-api]# docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                        NAMES\nf38807176f44        lamber/shop-api     \"/usr/bin/supervisord\"   7 seconds ago       Up 6 seconds        0.0.0.0:8022->22/tcp, 0.0.0.0:88->5000/tcp   shop-api\na1efc632ba6d        mynginx:v2          \"nginx\"                  5 hours ago         Up 5 hours          0.0.0.0:82->80/tcp                           mynginxv2\n604ac1c6443a        nginx/mynginx:v1    \"nginx\"                  6 hours ago         Up 6 hours          0.0.0.0:81->80/tcp                           mynginxv1\n\n```\n测试结果：\n\n```\n[root@linux-node1 shop-api]# curl 127.0.0.1:88\nHello World!\n[root@linux-node1 shop-api]# /root/docker_in.sh shop-api\n[root@f38807176f44 /]# ps aux\nUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot          1  0.0  0.5 117256 14796 ?        Ss   15:21   0:00 /usr/bin/python /usr/bin/supervisord -c /etc/supervisord.con\nroot          7  0.0  0.1  82524  3592 ?        S    15:21   0:00 /usr/sbin/sshd -D\nwww           8  0.0  0.6 119760 17328 ?        S    15:21   0:00 /usr/bin/python2.7 /opt/app.py\nwww          13  0.6  0.6 196036 18052 ?        Sl   15:21   0:02 /usr/bin/python2.7 /opt/app.py\nroot         19  0.0  0.0  15172  1896 ?        S    15:27   0:00 -bash\nroot         32  0.0  0.0  50844  1700 ?        R+   15:27   0:00 ps aux\n\n```\n访问容器的22端口：\n\n密码就是我们在创建centos-ssh时候指定的root：redhat\n\n![](http://omk1n04i8.bkt.clouddn.com/17-3-31/36142994-file_1490974494177_17425.jpg)\n#### 总结\n这就是分层的便利性，生产情况下都是使用supervisor来启动，即使是一个也要用supervisor来启动，python的依赖文件要写好。实际生产过程中，需要在真机上测试好了然后再进行镜像的封装。\n\n```\n[root@linux-node1 docker]# tree\n.\n├── app\n│   ├── shop-api\n│   │   ├── app.py\n│   │   ├── app-supervisor.ini\n│   │   ├── Dockerfile\n│   │   ├── requirements.txt\n│   │   └── supervisord.conf\n│   ├── xxx.admin\n│   └── xxx.api\n├── runtime\n│   ├── java\n│   ├── php\n│   ├── python\n│   │   └── Dockerfile\n│   └── python-ssh\n│       └── Dockerfile\n└── system\n    ├── centos\n    │   ├── Dockerfile\n    │   └── epel.repo\n    ├── centos-ssh\n    │   ├── Dockerfile\n    │   └── epel.repo\n    └── ubuntu\n\n```\n\n### 使用supervisor管理进程\n#### 参考文档\n\n> http://supervisord.org/\n>\n> http://liyangliang.me/posts/2015/06/using-supervisor/\n\n\n\n安装supervisor\n\n```\n[root@linux-node1 ~]# yum install supervisor -y\n[root@linux-node1 ~]# rpm -ql supervisor | head -2\n/etc/logrotate.d/supervisor\n/etc/supervisord.conf\n[root@linux-node1 ~]# tail -2 /etc/supervisord.conf \n[include]\nfiles = supervisord.d/*.ini\n\n```\n\n\n\n\n```\n[program:sshd]\ncommand=/usr/sbin/sshd -D\nprocess_name=%(program_name)s\nautostart=true\n```\n\n\n\n```\n# docker容器统一使用supervisor进行管理，是管理操作标准化起来。\n[root@6dd90ccf92d0 ~]# ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:01 ?        00:00:00 /usr/bin/python /usr/bin/supervisord -c /\nroot         7     1  0 18:01 ?        00:00:00 /usr/sbin/sshd -D\nroot         8     7  0 18:02 ?        00:00:00 sshd: root@pts/0\nroot        10     8  0 18:02 pts/0    00:00:00 -bash\nroot        23    10  0 18:03 pts/0    00:00:00 ps -ef\n[root@6dd90ccf92d0 ~]# supervisorctl status\nsshd                             RUNNING   pid 7, uptime 0:01:28\n```\n\n","timestamp":1528078404016},{"name":"02-Docker-registetry.md","path":"02-容器云/02-Docker进阶/02-Docker-registetry.md","content":"# Docker-Registry\n\n> DockerFile建议通过git来保存，\n\n\n\n实验环境可以操作的，生产不建议这样去操作的内容：\n\n- 容器停止后就自动删除：docker run —rm centos /bin/echo \"hello world\"\n- 杀死所有正在运行的服务器：docker kill $(docker ps -a -q)\n- 删除所有已经停止的容器：docker rm $(docker ps -a -q)\n- 删除所有未打标签的镜像：docker rmi $(docker images -q -f dangling=true)\n\n\n\n\n\nDocker registry功能比较单一，没有web界面。\n\nNginx + Docker registry 验证https（自签名证书）：\n\n1. 手动创建证书\n\n```shell\n[root@localhost registry]# openssl req -x509 -days 3650 -nodes -newkey rsa:2048 -keyout ./docker-registry.key -out ./docker-registry.crt\nGenerating a 2048 bit RSA private key\n...............................................................+++\n...+++\nwriting new private key to \'./docker-registry.key\'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter \'.\', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server\'s hostname) []:reg.unixhot.com\nEmail Address []:\n[root@localhost registry]# ll\n总用量 8\n-rw-r--r--. 1 root root 1289 5月  11 20:39 docker-registry.crt\n-rw-r--r--. 1 root root 1708 5月  11 20:39 docker-registry.key\n```\n\n2. 安装httpd-tools htpasswd实现验证功能\n\n```shell\n# htpasswd是httpd-tools工具集中的工具，因此首先要安装httpd-tools\nyum install -y httpd-tools\n# 创建使用-c参数，加密码不要使用-c，指定用户为demo\n[root@localhost registry]# htpasswd -c /opt/registry/docker-registry.htpasswd demo\nNew password: \nRe-type new password: \nAdding password for user demo\n```\n\n3. nginx proxy https  （有一个现成的镜像），启动一个docker-registry的容器，proxy到这里。\n\n```shell\n# --link可以让容器之间可以互访\ndocker run -d -p 443:443 \\\n--name docker-registry-proxy \\\n-e REGISTRY_HOST=\"docker-registry\" \\\n-e REGISTRY_PORT=\"5000\" \\\n-e SERVER_NAME=\"reg.unixhot.com\" \\\n--link docker-registry:docker-registry \\\n-v /opt/registry/docker-registry.htpasswd:/etc/nginx/.htpasswd:ro \\\n-v /opt/registry:/etc/nginx/ssl:ro \\\ncontainersol/docker-registry-proxy\n```\n\n4. docker 配置使用证书\n\n```shell\n# 在/etc/hosts中针对reg.unixhot.com做域名的映射\n192.168.56.101  reg.unixhot.com\n# 配置docker使用证书，如果说不买证书的话那么所有的需要连接的服务器都要配置这一部分\n# 将自签名证书放到/etc/docker下面\ncd /etc/docker\nmkdir -p certs.d/reg.unixhot.com\ncd certs.d/reg.unixhot.com\ncp /opt/registry/docker-registry.crt ca.crt\n# 检测docker是否可以进行登录\n[root@localhost ~]# docker login reg.unixhot.com\nUsername: demo\nPassword: \nLogin Succeeded\n```\n\n5. 登录，push镜像\n\n```shell\n# Push镜像第一件需要做的事情就是打标签，这个标签是给docker入库我们创建这个registry的时候打的标签\n[root@localhost ~]# docker tag unixhot/centos reg.unixhot.com/unixhot/centos\n# push镜像到仓库\n[root@localhost ~]# docker push reg.unixhot.com/unixhot/centos\nThe push refers to repository [reg.unixhot.com/unixhot/centos]\n1e11f9cfa6aa: Pushed \n43e653f84b79: Pushed \nlatest: digest: sha256:e0bf9b8009fdea481f4546d7139aa3beeabbe799d489843f1c6a61339ef11271 size: 3768\n```\n\n6. 查看，传上去了以后只能通过提供的api进行查看。\n\n```shell\n# 使用docker images也能查看到\n[root@localhost ~]# curl -X GET https://demo:demo@reg.unixhot.com/v2/_catalog -k\n{\"repositories\":[\"unixhot/centos\"]}\n```\n\n\n\n如果是购买的证书的话只需要上述的第二步和第三步以及第五步即可。手动创建和配置docker使用证书就过了。\n\n\n\n```shell\n[root@localhost opt]# mkdir registry\n[root@localhost opt]# cd registry/\n[root@localhost registry]# docker run -d --name docker-registry -v /opt/registry/:/tmp/registry-dev registry:2.2.1\nUnable to find image \'registry:2.2.1\' locally\n2.2.1: Pulling from library/registry\n8387d9ff0016: Pull complete \n3b52deaaf0ed: Pull complete \n4bd501fad6de: Pull complete \na3ed95caeb02: Pull complete \nb1f99b5938be: Pull complete \n82c34c0ec017: Pull complete \n5426c0c1c293: Pull complete \nDigest: sha256:30adb707d1b4d2ad694c38da3ea1e7d303fdbdce2538ab0372afe6f1523ae3c8\nStatus: Downloaded newer image for registry:2.2.1\n4b19999b188ce8ed1a0ea7eab52c36a9a3e17ce78147f24a99d04d39624d9d87\n```\n\n\n\n## 使用Habor构建企业级Docker Registry\n\n>Harbor是一个企业级Registry服务。它对开源的Docker Registry服务进行了扩展，添加了更多企业用户需要的功能。Harbor被设计用于部署一套组织内部使用的私有环境，这个私有Registry服务对于非常关心安全 的组织来说是十分重要的。另外，私有Registry服务可以通过避免从公域网下载镜像而提高企业生产力。这对于没有良好的Internet连接状态，使用Docker Container的用户是一个福音。\n>\n>Harbor是VMware公司最近开源的企业级Docker Registry项目(https://github.com/vmware/harbor) 。其目标是帮助用户迅速搭建一个企业级的Docker registry服务。它提供了管理UI, 基于角色的访问控制(Role Based Access Control)，AD/LDAP集成、以及审计日志(Audit logging) 等企业用户需求的功能，同时还原生支持中文。Harbor的每个组件都是以Docker容器的形式构建的，使用Docker Compose来对它进行部署。\n>\n>Harbor项目使用了go语言开发，WEB框架采用beego。容器应用的开发和运行离不开可靠的镜像管理。从安全和效率等方面考虑，在企业私有环境内部署的Registry服务是非常必要的。\n>\n>Harbor(https://github.com/vmware/harbor)由VMware中国研发团队为企业用户设计的Registry Server开源项目，包括了权限管理(RBAC)、图形管理界面、LDAP/AD集成、审计、自我注册、HA等企业必需的功能，同时针对中国用户的特点，原生支持中文，并计划实现镜像复制(roadmap)等功能。\n>\n>可参考内容：http://www.jiagoumi.com/work/1221.html\n\n### 安装Docker-compose\n\n```shell\n# 安装过程中要用到docker-compose，这个玩意是用python写的，所以用pip来安装\nyum -y install python-pip\npip install docker-compose\n```\n\n由于国内网速的问题这里我下载了harbor的离线包，离线包包含了所需的所有需要在线下载的镜像包，省去了很多麻烦的事情。[官方下载点](https://github.com/vmware/harbor/releases)&[百度云下载点](https://pan.baidu.com/s/1Ndp8Z_SHVD5AQ9J7ir7HYg)【提取码：5n52】\n\n安装过程就异常简单了，我们可以照着[官方文档](https://github.com/vmware/harbor/blob/master/docs/installation_guide.md)直接去操作，官方文档详细的介绍了，我们在安装过程中需要调整的内容。这里其实就是简单的设置一下\n\n### 修改harbor.cfg\n\n这里我只简单的修改了3个配置，具体的配置在官方文档中，有非常详细的介绍，详细到介绍每一个配置项，大家可以根据自己的需要去仔细查看\n\n```shell\n# 修改harbor管理台UI的访问地址，不要用localhost或者127.0.0.1这种本地的地址，因为要对外提供服务\nhostname = reg.unixhot.com\n# 配置访问的方式为https，如果是自己在测试机上搞的话那么可以使用http，但是生产建议务必使用https\nui_url_protocol = https\n# 如果有需要的话修改一下harbor的admin登录密码\nharbor_admin_password = Harbor12345\n```\n\n初始化相关配置文件：\n\n```shell\n./prepare\n```\n\n运行install脚本进行安装部署harbor：\n\n```shell\n# 在线安装的过程太痛苦了，直接clone下来的包docker-compose.yml中不知道为啥，所有registry里面的镜像的tag都是__version__导致下载的时候直接找不到对应的version，不知道是不是只能手动改才行。\n./install.sh\n```\n\n至此为止安装过程就结束了，离线版本包的安装是非常简单的，一步一步的操作即可，管理的话需要使用docker-compose来进行管理。\n\n### 配置SSL\n\n> 这里模拟使用https来进行访问，目前并没有购买证书，因此我们自己只能创建自签名证书来模拟这个环境。关于自签名证书的配置官方也是有教程的，直接按照教程去做就可以了。[跳转链接](https://github.com/vmware/harbor/blob/master/docs/configure_https.md)\n\n具体的说明就不在这里赘述了，要看说明可以看官方文档，这里只是体现命令\n\n```shell\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt\nopenssl req -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key -out reg.unixhot.com.csr\nopenssl x509 -req -days 365 -in reg.unixhot.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out reg.unixhot.com.crt\n```\n\n查看harbor.cfg配置文件中指定的ssl证书的位置：\n\n```shell\n#The path of cert and key files for nginx, they are applied only the protocol is set to https\nssl_cert = /data/cert/server.crt\nssl_cert_key = /data/cert/server.key\n```\n\n由于是默认配置，我也就没改，接下来就是把我们生成的key放到/data/cert/下并且按照配置文件中的重命名为server，注意这里是可以修改的，我图省事没有去动配置文件。\n\n```shell\n[root@localhost ~]# ll /data/cert/\ntotal 8\n-rw-r--r--. 1 root root 1866 May 12 13:22 server.crt\n-rw-r--r--. 1 root root 3272 May 12 13:22 server.key\n```\n\n刷新配置文件：\n\n```shell\n./prepare\ndocker-compose down\n# 启动harbor\ndocker-compose up -d\n```\n\n测试一下：\n\n```shell\n# 这里测试成功以后我们也就可以在网页端访问测试了。\n[root@localhost harbor]# docker login reg.unixhot.com \nUsername (admin): admin\nPassword: \nLogin Succeeded\n```\n\n### Push镜像测试\n\n将server.crt复制到/etc/docker/cert.d/reg.unixhot.com目录下，目录没有自己新建一个：\n\n```shell\n[root@localhost reg.unixhot.com]# pwd\n/etc/docker/certs.d/reg.unixhot.com\n[root@localhost reg.unixhot.com]# ll\ntotal 4\n-rw-r--r--. 1 root root 1866 May 12 14:25 server.crt\n```\n\n在web端Harbor中新建一个项目明为exercise\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-22/97293193.jpg)\n\npush镜像：\n\n```shell\n[root@localhost reg.unixhot.com]# docker login reg.unixhot.com -uadmin -pHarbor12345\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n# 首先记得打标签，docker tag src dst\n# dockerhub域名/项目名/镜像名:TAG\n[root@localhost reg.unixhot.com]# docker tag unixhot/centos reg.unixhot.com/exercise/centos\n[root@localhost reg.unixhot.com]# docker push reg.unixhot.com/exercise/centos\nThe push refers to repository [reg.unixhot.com/exercise/centos]\n1e11f9cfa6aa: Pushed \n43e653f84b79: Pushed \nlatest: digest: sha256:792ead7dcec256fc00102cd0dd505c11b54a906ff2df2c090be972a92976ebc7 size: 741\n```\n\n在网页端查看，就能看到我们的镜像了。\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-22/51015014.jpg)\n\n从Harbor把镜像给Pull下来测试：\n\n```shell\n[root@localhost ~]# docker pull reg.unixhot.com/exercise/centos:latest\nlatest: Pulling from exercise/centos\n469cfcc7a4b3: Already exists \n6863d4929975: Pull complete \nDigest: sha256:792ead7dcec256fc00102cd0dd505c11b54a906ff2df2c090be972a92976ebc7\nStatus: Downloaded newer image for reg.unixhot.com/exercise/centos:latest\n```\n\n测试成功。","timestamp":1528078404016},{"name":"01-k8s介绍和环境准备.md","path":"02-容器云/03-K8S手动部署/01-k8s介绍和环境准备.md","content":"# k8s介绍和环境准备\n\n> Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。参考内容：https://github.com/unixhot/salt-kubernetes\n>\n> 使用Kubernetes可以：\n>\n> - 自动化容器的部署和复制\n> - 随时扩展或收缩容器规模\n> - 将容器组织成组，并且提供容器间的负载均衡\n> - 很容易地升级应用程序容器的新版本\n> - 提供容器弹性，如果容器失效就替换它，等等...\n\n\n\nK8S架构\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-23/5948708.jpg)\n\n\n\n\n\nK8s物理架构图\n\n![](http://omk1n04i8.bkt.clouddn.com/18-5-24/38069438.jpg)\n\n\n\n## 基础环境准备\n\n| FQDN                    | IP地址         | 硬件                |\n| ----------------------- | -------------- | ------------------- |\n| linux-node1.example.com | 192.168.56.101 | 1C，2G内存，50G硬盘 |\n| linux-node2.example.com | 192.168.56.102 | 1C，2G内存，50G硬盘 |\n| linux-node3.example.com | 192.168.56.103 | 1C，2G内存，50G硬盘 |\n\n### 环境准备前提\n\n- 安装Centos7的64位操作系统\n- 基本系统：1核CPU，2G内存，50G硬盘\n  1. 网络选择：使用NAT\n  2. 软件包选择：最小化安装\n  3. 关闭iptables和selinux\n- 设置所有节点的主机名和ip地址，做好本地的主机名解析。\n\n```shell\n# 设置主机名\nhostnamectl set-hostname linux-node1.example.com\n[root@linux-node3 ~]# hostnamectl --static\nlinux-node3.example.com\n[root@linux-node3 ~]# hostnamectl --transient\nlinux-node3.example.com\n[root@linux-node3 ~]# hostname\nlinux-node3.example.com\n\n# 关闭防火墙和selinux\nsystemctl stop firewalld\nsystemctl disable firewalld\nsetenforce 0\nsed \"s/enforcing/disabled/g\" /etc/selinux/config  -i\n\n# 针对hosts做解析\necho -e \"192.168.56.101   linux-node1.example.com\\n192.168.56.102   linux-node2.example.com\\n192.168.56.103   linux-node3.example.com\" >> /etc/hosts\n```\n\n### 安装文件准备\n\n#### 安装Docker\n\n略，可以参照Docker部署这一小节，同时替换掉Docker的源为自己的阿里云加速源，不过说一个小问题，就是启动docker的时候可能会遇到这么一个报错，一般会遇到这个报错是因为你是直接yum安装的，而不是按照官方网站提供的办法安装的，版本不一致。\n\n```\nError starting daemon: SELinux is not supported with the overlay2 graph driver on this kernel. Either boot into a newer kernel or disable selinux in docker (--selinux-enabled=false)\n```\n\n意思是此linux的内核中的SELinux不支持 overlay2 graph driver ，解决方法有两个，要么启动一个新内核，要么就在docker里禁用selinux，--selinux-enabled=false，按照提示说的，我们修改一下参数：\n\n```shell\n[root@linux-node2 ~]# vim /etc/sysconfig/docker\n# /etc/sysconfig/docker\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS=\'--selinux-enabled=false --log-driver=journald --signature-verification=false\'\nif [ -z \"${DOCKER_CERT_PATH}\" ]; then\n    DOCKER_CERT_PATH=/etc/docker\nfi\n```\n\n#### 准备部署目录\n\n```shell\nmkdir -p /opt/kubernetes/{cfg,bin,ssl,log}\n```\n\n#### 准备软件包\n\n```shell\n# 将解压后的所有的内容放到/usr/local/src下面\n[root@linux-node1 ~]# ll /usr/local/src/\ntotal 599096\n-rw-r--r--. 1 root root   6595195 Mar 30  2016 cfssl-certinfo_linux-amd64\n-rw-r--r--. 1 root root   2277873 Mar 30  2016 cfssljson_linux-amd64\n-rw-r--r--. 1 root root  10376657 Mar 30  2016 cfssl_linux-amd64\n-rw-r--r--. 1 root root  17108856 Apr 12 17:35 cni-plugins-amd64-v0.7.1.tgz\n-rw-r--r--. 1 root root  10562874 Mar 30 01:58 etcd-v3.2.18-linux-amd64.tar.gz\n-rw-r--r--. 1 root root   9706487 Jan 24 02:58 flannel-v0.10.0-linux-amd64.tar.gz\n-rw-r--r--. 1 root root  13344537 Apr 13 01:51 kubernetes-client-linux-amd64.tar.gz\n-rw-r--r--. 1 root root 112427817 Apr 13 01:51 kubernetes-node-linux-amd64.tar.gz\n-rw-r--r--. 1 root root 428337777 Apr 13 01:51 kubernetes-server-linux-amd64.tar.gz\n-rw-r--r--. 1 root root   2716855 Apr 13 01:51 kubernetes.tar.gz\n```\n\n如果是生产环境的话可以直接去github上面去直接下载。\n\n```\ngithub的项目：https://github.com/kubernetes/kubernetes\nCHANGE_LOG_1.10：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md\n```\n\n#### 解压和环境变量\n\n```shell\n# 把我们下载下来的包进行解压的操作，然后设置一下我们的环境变量。\ncd /usr/local/src/\ntar zxf kubernetes.tar.gz \ntar zxf kubernetes-client-linux-amd64.tar.gz \ntar zxf kubernetes-node-linux-amd64.tar.gz \ntar zxf kubernetes-server-linux-amd64.tar.gz\n\n# 设置环境变量\nvim /root/.bash_profile\nPATH=$PATH:$HOME/bin:/opt/kubernetes/bin\n# 让环境变量生效\nsource /root/.bash_profile\n```\n\n","timestamp":1528078404016},{"name":"02-CA证书制作.md","path":"02-容器云/03-K8S手动部署/02-CA证书制作.md","content":"# CA证书制作\n\n## CA证书创建和分发\n\n> 从1.8开始部署k8s都要使用TLS证书对通信进行加密。\n>\n> CA证书管理（自签名）：\n>\n> - easyrsa\n> - openssl\n> - cfssl（这里选用cfssl是因为cfssl相对简单，使用json文件进行管理）\n\n### 1、安装CFSSL\n\n```shell\n# 如果要下载的话可以去这里下载：http://pkg.cfssl.org/\ncd /usr/local/src\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64\nchmod +x cfssl*\nmv cfssl-certinfo_linux-amd64 /opt/kubernetes/bin/cfssl-certinfo\nmv cfssljson_linux-amd64  /opt/kubernetes/bin/cfssljson\nmv cfssl_linux-amd64  /opt/kubernetes/bin/cfssl\n\n# 复制cfssl命令文件到k8s-node1和k8s-node2节点。如果实际中多个节点，就都需要同步复制。\nscp /opt/kubernetes/bin/cfssl* 192.168.56.102: /opt/kubernetes/bin\nscp /opt/kubernetes/bin/cfssl* 192.168.56.103: /opt/kubernetes/bin\n```\n\n### 2、初始化cfssl\n\n```shell\n# /usr/local/src目录下\nmkdir ssl && cd ssl\ncfssl print-defaults config > config.json\ncfssl print-defaults csr > csr.json\n\n[root@linux-node1 ssl]# ll\ntotal 8\n-rw-r--r--. 1 root root 567 May 13 05:27 config.json\n-rw-r--r--. 1 root root 287 May 13 05:27 csr.json\n```\n\n### 3、创建用来生成CA文件的JSON配置文件\n\n```json\n# /usr/local/src/ssl目录下，配置ca证书的话肯定有很多选项，cfssl的方式是以读取json文件的方式拿到这些选项。\n[root@linux-node1 ssl]# vim ca-config.json\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"8760h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\n            \"signing\",\n            \"key encipherment\",\n            \"server auth\",\n            \"client auth\"\n        ],\n        \"expiry\": \"8760h\"\n      }\n    }\n  }\n}\n```\n\n### 4、创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件\n\n```json\n[root@linux-node1 ssl]# vim ca-csr.json\n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n### 5、生成CA证书（ca.pem）和密钥（ca-key.pem）\n\n```shell\n[root@ linux-node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca\n[root@ linux-node1 ssl]# ls -l ca*\n-rw-r--r-- 1 root root  290 Mar  4 13:45 ca-config.json\n-rw-r--r-- 1 root root 1001 Mar  4 14:09 ca.csr\n-rw-r--r-- 1 root root  208 Mar  4 13:51 ca-csr.json\n-rw------- 1 root root 1679 Mar  4 14:09 ca-key.pem\n-rw-r--r-- 1 root root 1359 Mar  4 14:09 ca.pem\n```\n\n### 6、分发证书\n\n```shell\n# /usr/local/src/ssl目录下\ncp ca.csr ca.pem ca-key.pem ca-config.json /opt/kubernetes/ssl\n# SCP证书到k8s-node1和k8s-node2节点\nscp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.102:/opt/kubernetes/ssl \nscp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.103:/opt/kubernetes/ssl\n```\n\n\n\n","timestamp":1528078404016},{"name":"03-ETCD集群部署.md","path":"02-容器云/03-K8S手动部署/03-ETCD集群部署.md","content":"# ETCD集群部署\n\n> etcd集群是整个k8s集群中负责存储的部分，所以这一块要先有。\n>\n> etcd的github地址：https://github.com/coreos/etcd\n\n## 准备etcd的软件包\n\n```shell\n# 首先可以直接去这里去下载\nwget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz\n\n# 解压并处理\n[root@linux-node1 src]# tar zxf etcd-v3.2.18-linux-amd64.tar.gz\n[root@linux-node1 src]# cd etcd-v3.2.18-linux-amd64\n[root@linux-node1 etcd-v3.2.18-linux-amd64]# cp etcd etcdctl /opt/kubernetes/bin/\n\n# 分发到其他的两个node节点\n[root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.102:/opt/kubernetes/bin/\n[root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.103:/opt/kubernetes/bin/\n```\n\n## 创建etcd证书签名请求\n\n```shell\n# 注意这里hosts应该是node1就写node1的ip，node2就写node2的ip，但是这里为了批量通用分发，所以把所有的node节点ip都写上，便于分发。\n[root@linux-node1 ssl]# cat etcd-csr.json \n{\n  \"CN\": \"etcd\",\n  \"hosts\": [\n    \"127.0.0.1\",\n\"192.168.56.101\",\n\"192.168.56.102\",\n\"192.168.56.103\"\n  ],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n## 生成etcd证书和私钥\n\n```shell\n[root@linux-node1 ssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\\n-ca-key=/opt/kubernetes/ssl/ca-key.pem \\\n-config=/opt/kubernetes/ssl/ca-config.json \\\n-profile=kubernetes etcd-csr.json | cfssljson -bare etcd\n2018/05/13 07:12:23 [INFO] generate received request\n2018/05/13 07:12:23 [INFO] received CSR\n2018/05/13 07:12:23 [INFO] generating key: rsa-2048\n2018/05/13 07:12:24 [INFO] encoded CSR\n2018/05/13 07:12:24 [INFO] signed certificate with serial number 259443634666846653641933831011491882088863744942\n2018/05/13 07:12:24 [WARNING] This certificate lacks a \"hosts\" field. This makes it unsuitable for\nwebsites. For more information see the Baseline Requirements for the Issuance and Management\nof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);\nspecifically, section 10.2.3 (\"Information Requirements\").\n\n# 查看相关内容\n[root@linux-node1 ssl]# ll etcd*\n-rw-r--r--. 1 root root 1062 May 13 07:12 etcd.csr\n-rw-r--r--. 1 root root  290 May 13 07:09 etcd-csr.json\n-rw-------. 1 root root 1675 May 13 07:12 etcd-key.pem\n-rw-r--r--. 1 root root 1436 May 13 07:12 etcd.pem\n```\n\n## 将证书移动到/opt/kubernetes/ssl目录下\n\n```shell\n# 刚才操作的/usr/local/src/ssl目录下执行操作\ncp etcd*.pem /opt/kubernetes/ssl\nscp etcd*.pem 192.168.56.102:/opt/kubernetes/ssl\nscp etcd*.pem 192.168.56.103:/opt/kubernetes/ssl\n```\n\n## 配置ETCD的配置文件\n\n这里有很多的ip地址，分发到其他主机以后记得修改对应的ip地址，启动以后，会占用2379和2380两个端口，我们以后如果说k8s有什么问题的话排查端口就可以知道哪一个部分出现了问题了。\n\n```shell\n[root@linux-node1 ~]# vim /opt/kubernetes/cfg/etcd.conf\n#[member]\nETCD_NAME=\"etcd-node1\"\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n#ETCD_SNAPSHOT_COUNTER=\"10000\"\n#ETCD_HEARTBEAT_INTERVAL=\"100\"\n#ETCD_ELECTION_TIMEOUT=\"1000\"\nETCD_LISTEN_PEER_URLS=\"https://192.168.56.101:2380\"\nETCD_LISTEN_CLIENT_URLS=\"https://192.168.56.101:2379,https://127.0.0.1:2379\"\n#ETCD_MAX_SNAPSHOTS=\"5\"\n#ETCD_MAX_WALS=\"5\"\n#ETCD_CORS=\"\"\n#[cluster]\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://192.168.56.101:2380\"\n# if you use different ETCD_NAME (e.g. test),\n# set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\"\nETCD_INITIAL_CLUSTER=\"etcd-node1=https://192.168.56.101:2380,etcd-node2=https://192.168.56.102:2380,etcd-node3=https://192.168.56.103:2380\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\nETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\"\nETCD_ADVERTISE_CLIENT_URLS=\"https://192.168.56.101:2379\"\n#[security]\nCLIENT_CERT_AUTH=\"true\"\nETCD_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\"\nETCD_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\"\nETCD_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\"\nPEER_CLIENT_CERT_AUTH=\"true\"\nETCD_PEER_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\"\nETCD_PEER_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\"\nETCD_PEER_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\"\n```\n\n## 创建ETCD系统服务\n\ncentos7下面和centos6的系统启动管理脚本还是不太一样的，具体内容如下，设置完成以后不要着急启动，因为etcd会检查整个集群的健康状态，现在启动的话会一直卡住，等我们把所有的node节点都部署完成以后然后再进行启动。\n\n```shell\n[root@linux-node1 ~]# vim /etc/systemd/system/etcd.service\n[Unit]\nDescription=Etcd Server\nAfter=network.target\n\n[Service]\nType=simple\nWorkingDirectory=/var/lib/etcd\nEnvironmentFile=-/opt/kubernetes/cfg/etcd.conf\n# set GOMAXPROCS to number of processors\nExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) /opt/kubernetes/bin/etcd\"\nType=notify\n\n[Install]\nWantedBy=multi-user.target\n```\n\n## 重载系统服务\n\n```shell\n[root@linux-node1 ~]# systemctl daemon-reload\n[root@linux-node1 ~]# systemctl enable etcd\n```\n\n将配置文件分发到其他的节点并重载系统服务\n\n```shell\nscp /opt/kubernetes/cfg/etcd.conf 192.168.56.102:/opt/kubernetes/cfg/\nscp /etc/systemd/system/etcd.service 192.168.56.102:/etc/systemd/system/\nscp /opt/kubernetes/cfg/etcd.conf 192.168.56.103:/opt/kubernetes/cfg/\nscp /etc/systemd/system/etcd.service 192.168.56.103:/etc/systemd/system/\n```\n\n在所有节点上创建etcd存储目录并启动etcd\n\n```shell\n# 注意在执行这一部分之前务必要将其他的节点的etcd的配置文件部分的ip都改掉。\nmkdir /var/lib/etcd\nsystemctl start etcd\nsystemctl status etcd\n```\n\n\n\n## 验证集群\n\n```shell\n[root@linux-node1 ~]# etcdctl --endpoints=https://192.168.56.11:2379 \\\n  --ca-file=/opt/kubernetes/ssl/ca.pem \\\n  --cert-file=/opt/kubernetes/ssl/etcd.pem \\\n  --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-health\nmember 435fb0a8da627a4c is healthy: got healthy result from https://192.168.56.12:2379\nmember 6566e06d7343e1bb is healthy: got healthy result from https://192.168.56.11:2379\nmember ce7b884e428b6c8c is healthy: got healthy result from https://192.168.56.13:2379\ncluster is healthy\n```\n\n\n\n","timestamp":1528078404016},{"name":"04-K8S_master_node部署.md","path":"02-容器云/03-K8S手动部署/04-K8S_master_node部署.md","content":"## 1.部署Kubernetes API服务部署\n\n> Kubernetes的master节点除去我们部署好的etcd之外，我们还需要部署API Server，scheduler，controller manager这三个内容。来看一下这几个小内容的基本功能\n>\n> - apiserver提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等\n>   - 只有API Server才直接操作etcd\n>   - 其他模块通过API Server查询或修改数据\n>   - 提供其他模块之间的数据交互和通信的枢纽\n> - scheduler负责分配调度Pod到集群内的node节点\n>   - 监听kube-apiserver，查询还未分配Node的Pod\n>   - 根据调度策略为这些Pod分配节点\n> - controller-manager由一系列的控制器组成，它通过apiserver监控整个 集群的状态，并确保集群处于预期的工作状态\n\n### 0.准备软件包\n```\n[root@linux-node1 ~]# cd /usr/local/src/kubernetes\n[root@linux-node1 kubernetes]# cp server/bin/kube-apiserver /opt/kubernetes/bin/\n[root@linux-node1 kubernetes]# cp server/bin/kube-controller-manager /opt/kubernetes/bin/\n[root@linux-node1 kubernetes]# cp server/bin/kube-scheduler /opt/kubernetes/bin/\n```\n\n### 1.创建生成CSR的 JSON 配置文件\n```\n[root@linux-node1 src]# vim kubernetes-csr.json\n{\n  \"CN\": \"kubernetes\",\n  \"hosts\": [\n    \"127.0.0.1\",\n    \"192.168.56.11\",\n    \"10.1.0.1\", # 这个ip是干嘛的？\n    \"kubernetes\",\n    \"kubernetes.default\",\n    \"kubernetes.default.svc\",\n    \"kubernetes.default.svc.cluster\",\n    \"kubernetes.default.svc.cluster.local\"\n  ],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n### 2.生成 kubernetes 证书和私钥\n```\n [root@linux-node1 src]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\\n   -ca-key=/opt/kubernetes/ssl/ca-key.pem \\\n   -config=/opt/kubernetes/ssl/ca-config.json \\\n   -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes\n[root@linux-node1 src]# cp kubernetes*.pem /opt/kubernetes/ssl/\n[root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.12:/opt/kubernetes/ssl/\n[root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.13:/opt/kubernetes/ssl/\n```\n\n### 3.创建 kube-apiserver 使用的客户端 token 文件\n```\n[root@linux-node1 ~]#  head -c 16 /dev/urandom | od -An -t x | tr -d \' \'\nad6d5bb607a186796d8861557df0d17f \n[root@linux-node1 ~]# vim /opt/kubernetes/ssl/bootstrap-token.csv\nad6d5bb607a186796d8861557df0d17f,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"\n```\n\n### 4.创建基础用户名/密码认证配置\n```\n[root@linux-node1 ~]# vim /opt/kubernetes/ssl/basic-auth.csv\nadmin,admin,1\nreadonly,readonly,2\n```\n\n### 5.部署Kubernetes API Server\n```\n[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-apiserver.service\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/GoogleCloudPlatform/kubernetes\nAfter=network.target\n\n[Service]\nExecStart=/opt/kubernetes/bin/kube-apiserver \\\n  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\\n  --bind-address=192.168.56.11 \\\n  --insecure-bind-address=127.0.0.1 \\\n  --authorization-mode=Node,RBAC \\\n  --runtime-config=rbac.authorization.k8s.io/v1 \\\n  --kubelet-https=true \\\n  --anonymous-auth=false \\\n  --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \\\n  --enable-bootstrap-token-auth \\\n  --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \\\n  --service-cluster-ip-range=10.1.0.0/16 \\\n  --service-node-port-range=20000-40000 \\\n  --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\\n  --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\\n  --client-ca-file=/opt/kubernetes/ssl/ca.pem \\\n  --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\n  --etcd-cafile=/opt/kubernetes/ssl/ca.pem \\\n  --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\\n  --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \\\n  --etcd-servers=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \\\n  --enable-swagger-ui=true \\\n  --allow-privileged=true \\\n  --audit-log-maxage=30 \\\n  --audit-log-maxbackup=3 \\\n  --audit-log-maxsize=100 \\\n  --audit-log-path=/opt/kubernetes/log/api-audit.log \\\n  --event-ttl=1h \\\n  --v=2 \\\n  --logtostderr=false \\\n  --log-dir=/opt/kubernetes/log\nRestart=on-failure\nRestartSec=5\nType=notify\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 6.启动API Server服务\n```shell\n# 如果服务压根没起来要去/var/log/messages下去查看错误日志。\n# kube-api监控6443端口，本地127.0.0.1监听8080端口是给scheduler，controller-manager使用的。别人要想访问的话就必须通过6443，就必须做验证。\n\nsystemctl daemon-reload\nsystemctl enable kube-apiserver\nsystemctl start kube-apiserver\n```\n\n查看API Server服务状态\n```\n[root@linux-node1 ~]# systemctl status kube-apiserver\n```\n\n## 部署Controller Manager服务\n```\n[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/GoogleCloudPlatform/kubernetes\n\n[Service]\nExecStart=/opt/kubernetes/bin/kube-controller-manager \\\n  --address=127.0.0.1 \\\n  --master=http://127.0.0.1:8080 \\\n  --allocate-node-cidrs=true \\\n  --service-cluster-ip-range=10.1.0.0/16 \\\n  --cluster-cidr=10.2.0.0/16 \\\n  --cluster-name=kubernetes \\\n  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\n  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\n  --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\n  --root-ca-file=/opt/kubernetes/ssl/ca.pem \\\n  --leader-elect=true \\\n  --v=2 \\\n  --logtostderr=false \\\n  --log-dir=/opt/kubernetes/log\n\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 3.启动Controller Manager\n```\n# 监听本地的10252端口(127.0.0.1)\n[root@linux-node1 ~]# systemctl daemon-reload\n[root@linux-node1 scripts]# systemctl enable kube-controller-manager\n[root@linux-node1 scripts]# systemctl start kube-controller-manager\n```\n\n## 4.查看服务状态\n```\n[root@linux-node1 scripts]# systemctl status kube-controller-manager\n```\n\n## 部署Kubernetes Scheduler\n```\n[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-scheduler.service\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/GoogleCloudPlatform/kubernetes\n\n[Service]\nExecStart=/opt/kubernetes/bin/kube-scheduler \\\n  --address=127.0.0.1 \\\n  --master=http://127.0.0.1:8080 \\\n  --leader-elect=true \\\n  --v=2 \\\n  --logtostderr=false \\\n  --log-dir=/opt/kubernetes/log\n\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### 2.部署服务\n```\n# \n[root@linux-node1 ~]# systemctl daemon-reload\n[root@linux-node1 scripts]# systemctl enable kube-scheduler\n[root@linux-node1 scripts]# systemctl start kube-scheduler\n[root@linux-node1 scripts]# systemctl status kube-scheduler\n```\n\n## 部署kubectl 命令行工具\n\n1.准备二进制命令包\n```\n[root@linux-node1 ~]# cd /usr/local/src/kubernetes/client/bin\n[root@linux-node1 bin]# cp kubectl /opt/kubernetes/bin/\n```\n\n2.创建 admin 证书签名请求\n```\n[root@linux-node1 ~]# cd /usr/local/src/ssl/\n[root@linux-node1 ssl]# vim admin-csr.json\n{\n  \"CN\": \"admin\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n3.生成 admin 证书和私钥：\n```shell\n[root@linux-node1 ssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\\n   -ca-key=/opt/kubernetes/ssl/ca-key.pem \\\n   -config=/opt/kubernetes/ssl/ca-config.json \\\n   -profile=kubernetes admin-csr.json | cfssljson -bare admin\n[root@linux-node1 ssl]# ls -l admin*\n-rw-r--r-- 1 root root 1009 Mar  5 12:29 admin.csr\n-rw-r--r-- 1 root root  229 Mar  5 12:28 admin-csr.json\n-rw------- 1 root root 1675 Mar  5 12:29 admin-key.pem\n-rw-r--r-- 1 root root 1399 Mar  5 12:29 admin.pem\n\n[root@linux-node1 src]# cp admin*.pem /opt/kubernetes/ssl/\n```\n\n4.设置集群参数\n```shell\n[root@linux-node1 src]# kubectl config set-cluster kubernetes \\\n   --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n   --embed-certs=true \\\n   --server=https://192.168.56.11:6443\nCluster \"kubernetes\" set.\n```\n\n5.设置客户端认证参数\n```\n[root@linux-node1 src]# kubectl config set-credentials admin \\\n   --client-certificate=/opt/kubernetes/ssl/admin.pem \\\n   --embed-certs=true \\\n   --client-key=/opt/kubernetes/ssl/admin-key.pem\nUser \"admin\" set.\n```\n\n6.设置上下文参数\n```\n[root@linux-node1 src]# kubectl config set-context kubernetes \\\n   --cluster=kubernetes \\\n   --user=admin\nContext \"kubernetes\" created.\n```\n\n7.设置默认上下文\n```\n[root@linux-node1 src]# kubectl config use-context kubernetes\nSwitched to context \"kubernetes\".\n```\n\n做完了上面这一堆的配置以后其实是在家目录下的.kube目录下生成一个config文件，可以卡一下，kubectl就是通过这个文件来和kubeapiserver进行通信的。\n\n```shell\n[root@linux-node1 .kube]# ll config \n-rw------- 1 root root 6261 May 30 06:12 config\n[root@linux-node1 .kube]# pwd\n/root/.kube\n```\n\n8.使用kubectl工具\n\n```\n[root@linux-node1 ~]# kubectl get cs\nNAME                 STATUS    MESSAGE             ERROR\ncontroller-manager   Healthy   ok                  \nscheduler            Healthy   ok                  \netcd-1               Healthy   {\"health\":\"true\"}   \netcd-2               Healthy   {\"health\":\"true\"}   \netcd-0               Healthy   {\"health\":\"true\"}   \n```\n","timestamp":1528078404016},{"name":"05-Node节点部署.md","path":"02-容器云/03-K8S手动部署/05-Node节点部署.md","content":"## 部署kubelet\n\n1.二进制包准备\n将软件包从linux-node1复制到linux-node2中去。\n```\n[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/\n[root@linux-node1 bin]# cp kubelet kube-proxy /opt/kubernetes/bin/\n[root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.12:/opt/kubernetes/bin/\n[root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.13:/opt/kubernetes/bin/\n```\n\n2.创建角色绑定\n```\n[root@linux-node1 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap\nclusterrolebinding \"kubelet-bootstrap\" created\n```\n\n3.创建 kubelet bootstrapping kubeconfig 文件\n设置集群参数\n```\n[root@linux-node1 ~]# kubectl config set-cluster kubernetes \\\n   --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n   --embed-certs=true \\\n   --server=https://192.168.56.11:6443 \\\n   --kubeconfig=bootstrap.kubeconfig\nCluster \"kubernetes\" set.\n```\n\n设置客户端认证参数\n```\n[root@linux-node1 ~]# kubectl config set-credentials kubelet-bootstrap \\\n   --token=ad6d5bb607a186796d8861557df0d17f \\\n   --kubeconfig=bootstrap.kubeconfig   \nUser \"kubelet-bootstrap\" set.\n```\n\n设置上下文参数\n```\n[root@linux-node1 ~]# kubectl config set-context default \\\n   --cluster=kubernetes \\\n   --user=kubelet-bootstrap \\\n   --kubeconfig=bootstrap.kubeconfig\nContext \"default\" created.\n```\n\n选择默认上下文\n```\n[root@linux-node1 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig\nSwitched to context \"default\".\n\n# 生成bootstrap.kubconfig文件\n[root@linux-node1 kubernetes]# cp bootstrap.kubeconfig /opt/kubernetes/cfg\n[root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.12:/opt/kubernetes/cfg\n[root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.13:/opt/kubernetes/cfg\n```\n\n部署kubelet\n1.设置CNI支持\n```\n[root@linux-node2 ~]# mkdir -p /etc/cni/net.d\n[root@linux-node2 ~]# vim /etc/cni/net.d/10-default.conf\n{\n        \"name\": \"flannel\",\n        \"type\": \"flannel\",\n        \"delegate\": {\n            \"bridge\": \"docker0\",\n            \"isDefaultGateway\": true,\n            \"mtu\": 1400\n        }\n}\n\n\n```\n\n2.创建kubelet目录\n```\n[root@linux-node2 ~]# mkdir /var/lib/kubelet\n```\n\n3.创建kubelet服务配置\n```\n[root@k8s-node2 ~]# vim /usr/lib/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/GoogleCloudPlatform/kubernetes\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nWorkingDirectory=/var/lib/kubelet\nExecStart=/opt/kubernetes/bin/kubelet \\\n  --address=192.168.56.12 \\\n  --hostname-override=192.168.56.12 \\\n  --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\\n  --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\n  --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\n  --cert-dir=/opt/kubernetes/ssl \\\n  --network-plugin=cni \\\n  --cni-conf-dir=/etc/cni/net.d \\\n  --cni-bin-dir=/opt/kubernetes/bin/cni \\\n  --cluster-dns=10.1.0.2 \\\n  --cluster-domain=cluster.local. \\\n  --hairpin-mode hairpin-veth \\\n  --allow-privileged=true \\\n  --fail-swap-on=false \\\n  --logtostderr=true \\\n  --v=2 \\\n  --logtostderr=false \\\n  --log-dir=/opt/kubernetes/log\nRestart=on-failure\nRestartSec=5\n```\n\n4.启动Kubelet\n```\n[root@linux-node2 ~]# systemctl daemon-reload\n[root@linux-node2 ~]# systemctl enable kubelet\n[root@linux-node2 ~]# systemctl start kubelet\n```\n\n5.查看服务状态\n```\n[root@linux-node2 kubernetes]# systemctl status kubelet\n```\n\n6.查看csr请求\n注意是在linux-node1上执行。\n```\n[root@linux-node1 ~]# kubectl get csr\nNAME                                                   AGE       REQUESTOR           CONDITION\nnode-csr-0_w5F1FM_la_SeGiu3Y5xELRpYUjjT2icIFk9gO9KOU   1m        kubelet-bootstrap   Pending\n```\n\n7.批准kubelet 的 TLS 证书请求\n```\n# 批准以后会多一个kubernetes-client.crt这么一个证书\n[root@linux-node1 ~]# kubectl get csr|grep \'Pending\' | awk \'NR>0{print $1}\'| xargs kubectl certificate approve\n```\n执行完毕后，查看节点状态已经是Ready的状态了\n[root@linux-node1 ssl]#  kubectl get node\nNAME            STATUS    ROLES     AGE       VERSION\n\n## 部署Kubernetes Proxy\n1.配置kube-proxy使用LVS\n```\n[root@linux-node2 ~]# yum install -y ipvsadm ipset conntrack\n```\n\n2.创建 kube-proxy 证书请求\n```\n[root@linux-node1 ~]# cd /usr/local/src/ssl/\n[root@linux-node1 ~]# vim kube-proxy-csr.json\n{\n  \"CN\": \"system:kube-proxy\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n3.生成证书\n```\n[root@linux-node1~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\\n   -ca-key=/opt/kubernetes/ssl/ca-key.pem \\\n   -config=/opt/kubernetes/ssl/ca-config.json \\\n   -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy\n```\n4.分发证书到所有Node节点\n```\n[root@linux-node1 ssl]# cp kube-proxy*.pem /opt/kubernetes/ssl/\n[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/\n[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/\n```\n\n5.创建kube-proxy配置文件\n```\n[root@linux-node2 ~]# kubectl config set-cluster kubernetes \\\n   --certificate-authority=/opt/kubernetes/ssl/ca.pem \\\n   --embed-certs=true \\\n   --server=https://192.168.56.11:6443 \\\n   --kubeconfig=kube-proxy.kubeconfig\nCluster \"kubernetes\" set.\n\n[root@linux-node2 ~]# kubectl config set-credentials kube-proxy \\\n   --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\\n   --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\\n   --embed-certs=true \\\n   --kubeconfig=kube-proxy.kubeconfig\nUser \"kube-proxy\" set.\n\n[root@linux-node2 ~]# kubectl config set-context default \\\n   --cluster=kubernetes \\\n   --user=kube-proxy \\\n   --kubeconfig=kube-proxy.kubeconfig\nContext \"default\" created.\n\n[root@linux-node2 ~]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\nSwitched to context \"default\".\n```\n6.分发kubeconfig配置文件\n```\n[root@linux-node1 ssl]# cp kube-proxy.kubeconfig /opt/kubernetes/cfg/\n[root@linux-node1 ~]# scp kube-proxy.kubeconfig 192.168.56.12:/opt/kubernetes/cfg/\n[root@linux-node1 ~]# scp kube-proxy.kubeconfig 192.168.56.13:/opt/kubernetes/cfg/\n```\n\n7.创建kube-proxy服务配置\n```\n[root@linux-node2 bin]# mkdir /var/lib/kube-proxy\n\n[root@k8s-node2 ~]# vim /usr/lib/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Kube-Proxy Server\nDocumentation=https://github.com/GoogleCloudPlatform/kubernetes\nAfter=network.target\n\n[Service]\nWorkingDirectory=/var/lib/kube-proxy\nExecStart=/opt/kubernetes/bin/kube-proxy \\\n  --bind-address=192.168.56.12 \\\n  --hostname-override=192.168.56.12 \\\n  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \\\n--masquerade-all \\\n  --feature-gates=SupportIPVSProxyMode=true \\\n  --proxy-mode=ipvs \\\n  --ipvs-min-sync-period=5s \\\n  --ipvs-sync-period=5s \\\n  --ipvs-scheduler=rr \\\n  --logtostderr=true \\\n  --v=2 \\\n  --logtostderr=false \\\n  --log-dir=/opt/kubernetes/log\n\nRestart=on-failure\nRestartSec=5\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\n8.启动Kubernetes Proxy\n[root@linux-node2 ~]# systemctl daemon-reload\n[root@linux-node2 ~]# systemctl enable kube-proxy\n[root@linux-node2 ~]# systemctl start kube-proxy\n```\n\n9.查看服务状态\n查看kube-proxy服务状态\n```\n[root@linux-node2 scripts]# systemctl status kube-proxy\n\n检查LVS状态\n[root@linux-node2 ~]# ipvsadm -L -n\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.1.0.1:443 rr persistent 10800\n  -> 192.168.56.11:6443           Masq    1      0          0         \n```\n如果你在两台实验机器都安装了kubelet和proxy服务，使用下面的命令可以检查状态：\n```\n[root@linux-node1 ssl]#  kubectl get node\nNAME            STATUS    ROLES     AGE       VERSION\n192.168.56.12   Ready     <none>    22m       v1.10.1\n192.168.56.13   Ready     <none>    3m        v1.10.1\n```\nlinux-node3节点请自行部署。\n","timestamp":1528078404016},{"name":"06-flannel网络.md","path":"02-容器云/03-K8S手动部署/06-flannel网络.md","content":"- Flannel要使用etcd\n- Flannel给每一个node分配出来的ip地址段都不一样。\n\n1.为Flannel生成证书\n\n```\n[root@linux-node1 ~]# vim flanneld-csr.json\n{\n  \"CN\": \"flanneld\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\n```\n\n2.生成证书\n```\n[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\\n   -ca-key=/opt/kubernetes/ssl/ca-key.pem \\\n   -config=/opt/kubernetes/ssl/ca-config.json \\\n   -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld\n```\n3.分发证书\n```\n[root@linux-node1 ~]# cp flanneld*.pem /opt/kubernetes/ssl/\n[root@linux-node1 ~]# scp flanneld*.pem 192.168.56.12:/opt/kubernetes/ssl/\n[root@linux-node1 ~]# scp flanneld*.pem 192.168.56.13:/opt/kubernetes/ssl/\n```\n\n4.下载Flannel软件包\n```\n[root@linux-node1 ~]# cd /usr/local/src\n# wget\n https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz\n \n[root@linux-node1 src]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz\n[root@linux-node1 src]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/\n复制到linux-node2节点\n[root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.12:/opt/kubernetes/bin/\n[root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.13:/opt/kubernetes/bin/\n复制对应脚本到/opt/kubernetes/bin目录下。\n[root@linux-node1 ~]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/\n[root@linux-node1 bin]# cp remove-docker0.sh /opt/kubernetes/bin/\n[root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.12:/opt/kubernetes/bin/\n[root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.13:/opt/kubernetes/bin/\n```\n\n5.配置Flannel\n```\n[root@linux-node1 ~]# vim /opt/kubernetes/cfg/flannel\nFLANNEL_ETCD=\"-etcd-endpoints=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379\"\nFLANNEL_ETCD_KEY=\"-etcd-prefix=/kubernetes/network\"\nFLANNEL_ETCD_CAFILE=\"--etcd-cafile=/opt/kubernetes/ssl/ca.pem\"\nFLANNEL_ETCD_CERTFILE=\"--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem\"\nFLANNEL_ETCD_KEYFILE=\"--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem\"\n复制配置到其它节点上\n[root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.12:/opt/kubernetes/cfg/\n[root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.13:/opt/kubernetes/cfg/\n```\n\n6.设置Flannel系统服务\n```\n[root@linux-node1 ~]# vim /usr/lib/systemd/system/flannel.service\n[Unit]\nDescription=Flanneld overlay address etcd agent\nAfter=network.target\nBefore=docker.service\n\n[Service]\nEnvironmentFile=-/opt/kubernetes/cfg/flannel\nExecStartPre=/opt/kubernetes/bin/remove-docker0.sh\nExecStart=/opt/kubernetes/bin/flanneld ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE}\nExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker\n\nType=notify\n\n[Install]\nWantedBy=multi-user.target\nRequiredBy=docker.service\n复制系统服务脚本到其它节点上\n# scp /usr/lib/systemd/system/flannel.service 192.168.56.12:/usr/lib/systemd/system/\n# scp /usr/lib/systemd/system/flannel.service 192.168.56.13:/usr/lib/systemd/system/\n```\n\n## Flannel CNI集成\n下载CNI插件\n```\nhttps://github.com/containernetworking/plugins/releases\nwget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz\n[root@linux-node1 ~]# mkdir /opt/kubernetes/bin/cni\n[root@linux-node1 src]# tar zxf cni-plugins-amd64-v0.7.1.tgz -C /opt/kubernetes/bin/cni\n# scp -r /opt/kubernetes/bin/cni/* 192.168.56.12:/opt/kubernetes/bin/cni/\n# scp -r /opt/kubernetes/bin/cni/* 192.168.56.13:/opt/kubernetes/bin/cni/\n```\n创建Etcd的key\n```shell\n# 创建pod的网段是什么，要在etcd配置上，flannel会从etcd来取然后分配\n/opt/kubernetes/bin/etcdctl --ca-file /opt/kubernetes/ssl/ca.pem --cert-file /opt/kubernetes/ssl/flanneld.pem --key-file /opt/kubernetes/ssl/flanneld-key.pem \\\n      --no-sync -C https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \\\nmk /kubernetes/network/config \'{ \"Network\": \"10.2.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1 }}\' >/dev/null 2>&1\n```\n启动flannel\n```\n[root@linux-node1 ~]# systemctl daemon-reload\n[root@linux-node1 ~]# systemctl enable flannel\n[root@linux-node1 ~]# chmod +x /opt/kubernetes/bin/*\n[root@linux-node1 ~]# systemctl start flannel\n```\n查看服务状态\n```\n[root@linux-node1 ~]# systemctl status flannel\n```\n\n## 配置Docker使用Flannel\n```\n[root@linux-node1 ~]# vim /usr/lib/systemd/system/docker.service\n[Unit] #在Unit下面修改After和增加Requires\nAfter=network-online.target firewalld.service flannel.service\nWants=network-online.target\nRequires=flannel.service\n\n[Service] #增加EnvironmentFile=-/run/flannel/docker\nType=notify\nEnvironmentFile=-/run/flannel/docker\nExecStart=/usr/bin/dockerd $DOCKER_OPTS\n```\n将配置复制到另外两个阶段\n```\n# scp /usr/lib/systemd/system/docker.service 192.168.56.12:/usr/lib/systemd/system/\n# scp /usr/lib/systemd/system/docker.service 192.168.56.13:/usr/lib/systemd/system/\n```\n重启Docker\n```\n[root@linux-node1 ~]# systemctl daemon-reload\n[root@linux-node1 ~]# systemctl restart docker\n```\n\n## 创建第一个K8s应用\n\n1.创建一个测试用的deployment\n\n```\n[root@linux-node1 ~]# kubectl run net-test --image=alpine --replicas=2 sleep 360000\n```\n\n2.查看获取IP情况\n\n```\n[root@linux-node1 ~]# kubectl get pod -o wide\nNAME                        READY     STATUS    RESTARTS   AGE       IP          NODE\nnet-test-5767cb94df-q92md   1/1       Running   0          35s       10.2.40.2   192.168.56.13\nnet-test-5767cb94df-s76nb   1/1       Running   0          35s       10.2.79.2   192.168.56.12\n```\n\n3.测试联通性\n\n```\nping 10.2.40.2\nping 10.2.79.2\n```\n\n书写deployement\n\n```\n[root@linux-node1 ~]# vim nginx-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.10.3\n        ports:\n        - containerPort: 80\n```\n\n创建\n\n```\n[root@linux-node1 ~]# kubectl create -f nginx-deployment.yaml \ndeployment.apps \"nginx-deployment\" created\n\n[root@linux-node1 ~]# kubectl get deployment\nNAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnet-test           2         2         2            2           6m\nnginx-deployment   3         3         3            0           31s\n```\n\n查看详情：\n\n```\n[root@linux-node1 ~]# kubectl describe deployment nginx-deployment\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Wed, 30 May 2018 07:35:06 -0400\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=1\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 0 available | 3 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  app=nginx\n  Containers:\n   nginx:\n    Image:        nginx:1.10.3\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      False   MinimumReplicasUnavailable\n  Progressing    True    ReplicaSetUpdated\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-75d56bb955 (3/3 replicas created)\nEvents:\n  Type    Reason             Age   From                   Message\n  ----    ------             ----  ----                   -------\n  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set nginx-deployment-75d56bb955 to 3\n```\n\n查看pod\n\n```shel\nkubectl describe pod nginx-deployment-75d56bb955-djxmq\n```\n\n```\n[root@linux-node1 ~]# kubectl get pod\nNAME                                READY     STATUS    RESTARTS   AGE\nnet-test-5767cb94df-q92md           1/1       Running   0          10m\nnet-test-5767cb94df-s76nb           1/1       Running   0          10m\nnginx-deployment-75d56bb955-djxmq   1/1       Running   0          4m\nnginx-deployment-75d56bb955-dm4xs   1/1       Running   0          4m\nnginx-deployment-75d56bb955-l674f   1/1       Running   0          4m\n```\n\n```\n[root@linux-node1 ~]# kubectl get pod -o wide\nNAME                                READY     STATUS    RESTARTS   AGE       IP          NODE\nnet-test-5767cb94df-q92md           1/1       Running   0          11m       10.2.40.2   192.168.56.13\nnet-test-5767cb94df-s76nb           1/1       Running   0          11m       10.2.79.2   192.168.56.12\nnginx-deployment-75d56bb955-djxmq   1/1       Running   0          5m        10.2.40.4   192.168.56.13\nnginx-deployment-75d56bb955-dm4xs   1/1       Running   0          5m        10.2.40.3   192.168.56.13\nnginx-deployment-75d56bb955-l674f   1/1       Running   0          5m        10.2.79.3   192.168.56.12\n\n[root@linux-node1 ~]# curl --head http://10.2.40.4\nHTTP/1.1 200 OK\nServer: nginx/1.10.3\nDate: Wed, 30 May 2018 11:40:55 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 31 Jan 2017 15:01:11 GMT\nConnection: keep-alive\nETag: \"5890a6b7-264\"\nAccept-Ranges: bytes\n```\n\n更新deployment\n\n```shell\n# --record记录历史日志\nkubectl set image deployment/nginx-deployment nginx=nginx:1.12.2 --record\n```\n\n查看更新后的deployment\n\n```shell\nkubectl get deployment -o wide\n```\n\n查看更新历史\n\n```shell\nkubectl rollout history deployment/nginx-deployment\n```\n\n查看具体某一个版本的升级历史\n\n```shell\nkubectl rollout history deployment/nginx-deployment --revision=1\n```\n\n快速回滚到上一个版本\n\n```shell\nkubectl rollout undo deployment/nginx-deployment\n```\n\n在这个过程中会发现过程中每次pod的ip都会发生变化。因此不能去访问pod的ip，要去访问service：\n\n```\n[root@linux-node1 ~]# vim nginx-service.yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n    \n\n[root@linux-node1 ~]# kubectl create -f nginx-service.yaml \nservice \"nginx-service\" created\n\n\n# 这个拿到的就是vip，可以帮我们做负载均衡。\n# 记住只有node节点，装了kube-proxy的才能访问，比如你的k8s master没装kube-proxy那就不能访问的\n[root@linux-node1 ~]# kubectl get service\nNAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nkubernetes      ClusterIP   10.1.0.1      <none>        443/TCP   1h\nnginx-service   ClusterIP   10.1.225.31   <none>        80/TCP    29s\n\n# 当前副本是3个，快速扩容到5个\nkubectl scale deployment nginx-deployment --replicas 5\n```\n\n","timestamp":1528078404016},{"name":"07-Dashboard.md","path":"02-容器云/03-K8S手动部署/07-Dashboard.md","content":"# Kubernetes Dashboard\n\n## coredns.yaml\n\n```\n[root@linux-node1 ~]# cat coredns.yaml \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      kubernetes.io/cluster-service: \"true\"\n      addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: EnsureExists\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health\n        kubernetes cluster.local. in-addr.arpa ip6.arpa {\n            pods insecure\n            upstream\n            fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        proxy . /etc/resolv.conf\n        cache 30\n    }\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: coredns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      k8s-app: coredns\n  template:\n    metadata:\n      labels:\n        k8s-app: coredns\n    spec:\n      serviceAccountName: coredns\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n      containers:\n      - name: coredns\n        image: coredns/coredns:1.0.6\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: coredns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: coredns\n  clusterIP: 10.1.0.2\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n```\n\n## 创建CoreDNS\n```\n[root@linux-node1 ~]# kubectl create -f coredns.yaml \n\n[root@linux-node1 ~]# kubectl get pod -n kube-system\nNAME                                    READY     STATUS    RESTARTS   AGE\ncoredns-77c989547b-9pj8b                1/1       Running   0          6m\ncoredns-77c989547b-kncd5                1/1       Running   0          6m\n\n[root@linux-node1 ~]# kubectl get deployment -n kube-system\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ncoredns   2         2         2            2           2m\n\n[root@linux-node1 ~]# kubectl get service -n kube-system\nNAME      TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE\ncoredns   ClusterIP   10.1.0.2     <none>        53/UDP,53/TCP   3m\n\n# 在有kube-proxy的位置查看转发\n[root@linux-node2 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.1.0.1:443 rr persistent 10800\n  -> 192.168.56.11:6443           Masq    1      1          0         \nTCP  10.1.0.2:53 rr\n  -> 10.2.40.9:53                 Masq    1      0          0         \n  -> 10.2.79.8:53                 Masq    1      0          0         \nTCP  10.1.225.31:80 rr\n  -> 10.2.40.6:80                 Masq    1      0          0         \n  -> 10.2.40.7:80                 Masq    1      0          0         \n  -> 10.2.40.8:80                 Masq    1      0          0         \n  -> 10.2.79.6:80                 Masq    1      0          0         \n  -> 10.2.79.7:80                 Masq    1      0          0         \nUDP  10.1.0.2:53 rr\n  -> 10.2.40.9:53                 Masq    1      0          0         \n  -> 10.2.79.8:53                 Masq    1      0          0 \n  \n  \n[root@linux-node1 ~]# kubectl get pod -n kube-system       \nNAME                       READY     STATUS    RESTARTS   AGE\ncoredns-77c989547b-54hwk   1/1       Running   0          5m\ncoredns-77c989547b-gf54w   1/1       Running   0          5m\n\n\n# 起容器\n[root@linux-node1 ~]# kubectl run dns-test --rm -it --image=alpine /bin/sh \nIf you don\'t see a command prompt, try pressing enter.\n/ # ping dcgamer.top\nPING dcgamer.top (124.193.0.2): 56 data bytes\n64 bytes from 124.193.0.2: seq=0 ttl=127 time=7.602 ms\n64 bytes from 124.193.0.2: seq=1 ttl=127 time=5.979 ms\n64 bytes from 124.193.0.2: seq=2 ttl=127 time=5.164 ms\n64 bytes from 124.193.0.2: seq=3 ttl=127 time=5.314 ms\n\n# 查看日志\n[root@linux-node1 dashboard]# kubectl get pod -n kube-system\nNAME                                    READY     STATUS              RESTARTS   AGE\ncoredns-77c989547b-54hwk                1/1       Running             0          15m\ncoredns-77c989547b-gf54w                1/1       Running             0          15m\nkubernetes-dashboard-66c9d98865-spkfj   0/1       ContainerCreating   0          12s\n[root@linux-node1 dashboard]# kubectl logs pod/coredns-77c989547b-54hwk -n kube-system\n.:53\nCoreDNS-1.0.6\nlinux/amd64, go1.10, 83b5eadb\n2018/05/30 12:22:51 [INFO] CoreDNS-1.0.6\n2018/05/30 12:22:51 [INFO] linux/amd64, go1.10, 83b5eadb\n```\n\n## 创建Dashboard\n\n```\n[root@linux-node1 dashboard]# ll\ntotal 28\n-rw-r--r-- 1 root root  515 May 17 10:09 admin-token.yaml\n-rw-r--r-- 1 root root  357 May 17 06:26 admin-user-sa-rbac.yaml\n-rw-r--r-- 1 root root  330 May 17 10:07 dashboard-admin.yaml\n-rw-r--r-- 1 root root 4901 May 17 06:26 kubernetes-dashboard.yaml\n-rw-r--r-- 1 root root  458 May 17 06:26 ui-admin-rbac.yaml\n-rw-r--r-- 1 root root  477 May 17 06:26 ui-read-rbac.yaml\n\n[root@linux-node1 dashboard]# kubectl create -f .\nclusterrolebinding.rbac.authorization.k8s.io \"admin\" created\nserviceaccount \"admin\" created\nserviceaccount \"admin-user\" created\nclusterrolebinding.rbac.authorization.k8s.io \"admin-user\" created\nclusterrolebinding.rbac.authorization.k8s.io \"kubernetes-dashboard\" created\nsecret \"kubernetes-dashboard-certs\" created\nserviceaccount \"kubernetes-dashboard\" created\nrole.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\nrolebinding.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\ndeployment.apps \"kubernetes-dashboard\" created\nservice \"kubernetes-dashboard\" created\nclusterrole.rbac.authorization.k8s.io \"ui-admin\" created\nrolebinding.rbac.authorization.k8s.io \"ui-admin-binding\" created\nclusterrole.rbac.authorization.k8s.io \"ui-read\" created\nrolebinding.rbac.authorization.k8s.io \"ui-read-binding\" created\n\n\n# 监听了node port。映射到了node节点的25106，记得是node节点的25106，不是master，因为master没有起kube-proxy\n[root@linux-node1 dashboard]# kubectl get service -n kube-system\nNAME                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE\ncoredns                ClusterIP   10.1.0.2     <none>        53/UDP,53/TCP   18m\nkubernetes-dashboard   NodePort    10.1.13.1    <none>        443:25106/TCP   3m\n\n# 记得访问https的地址\nhttps://192.168.100.12:25106/\n```\n\n```\n[root@linux-node1 ~]# kubectl create -f dashboard/\n```\n\n```\n# 执行这个命令获取令牌，登录仪表盘，输入令牌\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \'{print $1}\')\n```\n\n","timestamp":1528078404016},{"name":"04-K8S自动化部署.md","path":"02-容器云/04-K8S自动化部署.md","content":"","timestamp":1528078404016}]